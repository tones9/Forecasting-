{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ODxct0t4VqeV"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# %% Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sktime.forecasting.trend import STLForecaster\n",
    "# from IPython import get_ipython\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "kZKZO4j_VvvF"
   },
   "outputs": [],
   "source": [
    "# get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "\n",
    "\n",
    "model_names_mapping_data = {\n",
    "\"lr\": \"Linear Regression\",\n",
    "    \"lasso\": \"Lasso Regression\",\n",
    "    \"ridge\": \"Ridge Regression\",\n",
    "    \"en\": \"Elastic Net\",\n",
    "    \"llars\": \"Least Angle\",\n",
    "    \"pa\": \"Passive Agressive\",\n",
    "    \"knn\": \"(K) Nearest Neighbours\",\n",
    "    \"cart\": \"Decision Trees\",\n",
    "    \"extra\": \"Extra Trees\",\n",
    "    \"svmr\": \"Support vector Machines\",\n",
    "    \"ada\": \"Adaboost\",\n",
    "    \"bag\": \"Bagging (Decision Trees)\",\n",
    "    \"rf\": \"Random Forest\",\n",
    "    \"gbm\": \"Gradient Boosting\",\n",
    "    \"AR\": \"Auto Regression (AR)\",\n",
    "    \"MA\": \"Moving Average (MA)\",\n",
    "    \"ARMA\": \"ARMA\",\n",
    "    \"ARIMA\": \"ARIMA\",\n",
    "    \"spl_ARIMA\": \"ARIMA (Growing Trend)\",\n",
    "    \"auto_ARIMA\": \"autoARIMA\",\n",
    "    \"SES\": \"Exponential Smoothing\",\n",
    "    \"HWES\": \"Holts Winters\",\n",
    "    \"naive\": \"Naive Forecast\",\n",
    "    \"naive_rept\": \"Cyclicity Naive\",\n",
    "    \"naive3\": \"Naive (1 month Lag)\",\n",
    "    \"naive6\": \"Seasonal Naive (6 Month lag)\",\n",
    "    \"naive12\": \"Periodic Model\",\n",
    "    \"naive12wa\": \"Weighted Naive\",\n",
    "    \"sma\": \"Moving Average (Seasonal)\",\n",
    "    \"wma\": \"Moving Average (Weighted)\",\n",
    "    \"STL_ARIMA\": \"Best Forecast\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "a5FBQx4qWB3n"
   },
   "outputs": [],
   "source": [
    "def change_name(output_all):\n",
    "    output_all.replace(\"STL_ARIMA\",\"Best Forecast\", inplace=True)\n",
    "    return output_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ANMSuNjdWFOq"
   },
   "outputs": [],
   "source": [
    "def model_names_mapping(name):\n",
    "    return (model_names_mapping_data[name] if name in model_names_mapping_data else name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "1DngylKiWNiU"
   },
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "# async def forecast_main(input_data_json):\n",
    "# if True:\n",
    "def user_input():\n",
    "    \"\"\"\n",
    "\n",
    "    This Function takes into the account for both types of Data input. Whether the data is in .csv or .json.\n",
    "\n",
    "    Future will try to input the excel as well\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    forecast_period = input(\"What is the Future Forecast period? \")\n",
    "    forecast_period = int(forecast_period)\n",
    "    input_data = input(\"Enter dataset: \")\n",
    "#    details=input(\"Enter details dataset: \")\n",
    "\n",
    "    file_type = os.path.splitext(input_data)[1]\n",
    "    if file_type == '.json':\n",
    "\n",
    "        dataset = retrieve_data(input_data)\n",
    "\n",
    "#        detailed_data=retrieve_details(details)\n",
    "    return forecast_period, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "voJ_HWx7WULW"
   },
   "outputs": [],
   "source": [
    "def retrieve_data(path):\n",
    "  datasets = dict()\n",
    "  with open(path) as json_file:\n",
    "      raw_data = json.load(json_file)\n",
    "      for element in raw_data:\n",
    "          df_name = element['sku']\n",
    "          data = pd.DataFrame()\n",
    "          data['time'] = element['time']\n",
    "          data['sales'] = element['sales']\n",
    "          data['sales'] = [np.nan if pd.isnull(i) else int(i) for i in data['sales']]\n",
    "          data = data.T\n",
    "          data = data.rename(columns = data.iloc[0]).drop(data.index[0])\n",
    "          datasets[df_name] = copy.deepcopy(data)\n",
    "\n",
    "  return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zn7073pPWdQb",
    "outputId": "bd4dc96d-ba4e-4477-fb58-1cb52941de7f"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the Future Forecast period?  12\n",
      "Enter dataset:  meta (1).json\n"
     ]
    }
   ],
   "source": [
    "forecast_period, datasets = user_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "vZ4O5ojGWkKe"
   },
   "outputs": [],
   "source": [
    "def init_output(forecast_period, raw_data):\n",
    "    output = {}\n",
    "    output['forecast_period'] = forecast_period\n",
    "    output['forecast_values'] = []\n",
    "    output['interval'] = 'M'\n",
    "    output['actuals'] = assign_dates(raw_data, 'actuals')\n",
    "    output['best_models_ml'] = []\n",
    "    output['best_models_ts'] = []\n",
    "    output['bias_ml'] = []\n",
    "    output['bias_ts'] = []\n",
    "    output['bias_en'] = []\n",
    "    output['accuracy_ml'] = []\n",
    "    output['accuracy_ts'] = []\n",
    "    output['accuracy_en'] = []\n",
    "    output['validation'] = dict()\n",
    "    output['validation_facc'] = dict()\n",
    "    output['facc'] = ''\n",
    "    output['mape'] = ''\n",
    "    output['bias'] = ''\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "-v6qGOy9Wok7"
   },
   "outputs": [],
   "source": [
    "def assign_dates(data, flag, dates='', sku=\"\"):  # changed\n",
    "    if flag == 'validation':\n",
    "        dates = dates.reset_index()\n",
    "        dates.columns = ['time', 'sales']\n",
    "        dates.time = pd.to_datetime(\n",
    "            dates.time, format='%m/%d/%y', infer_datetime_format=True)\n",
    "        dates.time = dates.time.dt.to_period('M')\n",
    "        data = [float(format(i, '.3f')) for i in data]\n",
    "        result = pd.DataFrame(\n",
    "            {'time': dates.time.astype(str), 'validation': data})\n",
    "        result.set_index('time', inplace=True)\n",
    "        result = result.to_dict()\n",
    "        result = result['validation']\n",
    "\n",
    "    elif flag == 'val_facc':\n",
    "        dates = dates.reset_index()\n",
    "        dates.columns = ['time', 'sales']\n",
    "        dates.time = pd.to_datetime(\n",
    "            dates.time, format='%m/%d/%y', infer_datetime_format=True)\n",
    "        dates.time = dates.time.dt.to_period('M')\n",
    "        result = pd.DataFrame(\n",
    "            {'time': dates.time.astype(str), 'val_facc': data})\n",
    "        result.set_index('time', inplace=True)\n",
    "        result = result.to_dict()\n",
    "        result = result['val_facc']\n",
    "\n",
    "    elif flag == 'forecast':\n",
    "        dates = dates.reset_index()\n",
    "        dates.columns = ['time', 'forecast']\n",
    "        last_date = pd.to_datetime(\n",
    "            dates.time[0], format='%m/%d/%y', infer_datetime_format=True)\n",
    "        date_range = pd.date_range(last_date, periods=int(\n",
    "            forecast_period), freq='M')  # changed1st\n",
    "        date_range = date_range.strftime('%Y-%m').tolist()\n",
    "        poped_date = date_range.pop(0)  # same month as last_date, FUSO\n",
    "        data = [float(format(i, '.3f')) for i in data]\n",
    "        if len(date_range) != len(data):\n",
    "            date_range.append(poped_date)\n",
    "            error_sku.append(sku)\n",
    "        result = pd.DataFrame({'time': date_range, 'forecast': data})\n",
    "        result.set_index('time', inplace=True)\n",
    "        result = result.to_dict()\n",
    "        result = result['forecast']\n",
    "\n",
    "    elif flag == 'actuals':\n",
    "        data = data.tail(8).reset_index()\n",
    "        data.columns = ['time', 'sales']\n",
    "        data['time'] = pd.to_datetime(\n",
    "            data['time'], format='%m/%d/%y', infer_datetime_format=True)\n",
    "        data['time'] = data['time'].dt.to_period('M').astype(str)\n",
    "        data['sales'] = [0 if pd.isnull(i) else int(i)\n",
    "                            for i in data['sales']]\n",
    "        data['sales'] = [float(format(i, '.3f')) for i in data['sales']]\n",
    "        data.set_index('time', inplace=True)\n",
    "        result = data.to_dict()\n",
    "        result = result['sales']\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "6gEf2gWjWuc4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_interval(date):\n",
    "    if \"-\" in date[0]:\n",
    "        date_format = \"%m-%d-%Y\"\n",
    "    elif \"/\" in date[0]:\n",
    "        date_format = \"%m/%d/%y\"\n",
    "    else:\n",
    "        date_format = \"%m %d %Y\"\n",
    "\n",
    "    date = pd.to_datetime(date, format=date_format,\n",
    "                            infer_datetime_format=True)\n",
    "    diff = []\n",
    "    for i in range(len(date)-1):\n",
    "        interval = date[i+1]-date[i]\n",
    "        diff.append(interval)\n",
    "    mode = statistics.mode(diff)\n",
    "\n",
    "    return mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "UUtGW8YbWzTx"
   },
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "\n",
    "def output_forecast(sku, dataset, sku_data, output, forecast_results):\n",
    "    dataset = dataset.reset_index()\n",
    "    dataset.columns = ['time', 'sales']\n",
    "\n",
    "    output['last_date'] = dataset.time.iloc[-1]\n",
    "\n",
    "    forecast_result = add_forecasted_results(\n",
    "        sku, dataset, sku_data, output)\n",
    "    forecast_results.append(forecast_result)\n",
    "\n",
    "    return forecast_results\n",
    "\n",
    "def add_forecasted_results(sku, dataset, data, output):\n",
    "    sku_data = dict()\n",
    "    sku_data['sku'] = sku\n",
    "\n",
    "    for key in output:\n",
    "        sku_data[key] = output[key]\n",
    "\n",
    "    return sku_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "EuQa6OOjW3dY"
   },
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "\n",
    "def calculate_forecast_accuracy(expected, forecast):\n",
    "    if math.isnan(expected):\n",
    "        expected = 0\n",
    "    else:\n",
    "        expected = int(expected)\n",
    "    expected = int(expected)\n",
    "    forecast = int(forecast)\n",
    "    print(\"calculate_forecast_accuracy\")\n",
    "    facc = (1 - (np.abs(expected - forecast)) /\n",
    "            (expected+(expected == 0))) * 100\n",
    "    if facc < 0:\n",
    "        facc = 0\n",
    "    mape = (np.abs(expected - forecast) / expected) * 100\n",
    "    bias = (expected - forecast)\n",
    "\n",
    "    if np.isnan(facc) == True or np.isfinite(facc) == False:\n",
    "        facc = 0\n",
    "    if np.isnan(mape) == True or np.isfinite(mape) == False:\n",
    "        mape = 0\n",
    "    if np.isnan(bias) == True or np.isfinite(bias) == False:\n",
    "        mape = 0\n",
    "    return float(format(facc, '.3f')), float(format(mape, '.3f')), float(format(bias, '.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "8xwvsea_W4Uy"
   },
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "def read_from_first_sales(sku_data):\n",
    "    test = pd.isnull(sku_data)\n",
    "    index = np.where(test == False)[0]\n",
    "    index = index[0]\n",
    "    sku_data = sku_data[index:]\n",
    "    sku_data = sku_data.reset_index(drop=True)\n",
    "    return sku_data\n",
    "\n",
    "def get_bucket_size(interval):\n",
    "    interval_type = find_interval_type(\n",
    "        interval)  # aggregation (weekly/monthly)\n",
    "    if interval_type == 'W':\n",
    "        pt = 12\n",
    "        sindex = 24\n",
    "        freq = 52\n",
    "    elif interval_type == 'M' or interval_type == 'Random':\n",
    "        pt = 6\n",
    "        sindex = 10\n",
    "        freq = 12\n",
    "    elif interval_type == 'Y':\n",
    "        pt = 2\n",
    "        sindex = 0\n",
    "        freq = 0\n",
    "    elif interval_type=='D':\n",
    "        pt = 30\n",
    "        sindex = 0\n",
    "        freq = 365\n",
    "    return pt, sindex, freq\n",
    "\n",
    "def find_interval_type(interval):\n",
    "    interval = interval.days\n",
    "    if interval == 7:\n",
    "        itype = 'W'\n",
    "    elif interval == 30 or interval == 31:\n",
    "        itype = 'M'\n",
    "    elif interval == 365:\n",
    "        itype = 'Y'\n",
    "    elif interval == 1:\n",
    "        itype = 'D'\n",
    "    else:\n",
    "        itype = 'Random'\n",
    "\n",
    "    return itype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "c9mr-A_5W7r4"
   },
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "\n",
    "def data_imputation_zero(dataset):\n",
    "    dataset.fillna(0, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def Moving_Average(data, tsize):\n",
    "    rmse = dict()\n",
    "    model_predictions = dict()\n",
    "    key = ['sma', 'wma']\n",
    "    train, test = data[0:-tsize], data[-tsize:]\n",
    "    test = pd.DataFrame(test)\n",
    "    test = test.reset_index(drop=True)\n",
    "#    expected = test\n",
    "    test_shape = len(test)\n",
    "    if len(key) > 0:\n",
    "        if len(key) == 2:\n",
    "            predictions, rmse_i = model_MA(\n",
    "                key[0], train, test_shape, train_flag=1)\n",
    "            rmse[key[0]] = rmse_i\n",
    "            model_predictions[key[0]] = predictions\n",
    "\n",
    "            predictions, rmse_i = model_MA(\n",
    "                key[1], train, test_shape, train_flag=1)\n",
    "            rmse[key[1]] = rmse_i\n",
    "            model_predictions[key[1]] = predictions\n",
    "        else:\n",
    "            predictions, rmse_i = model_MA(\n",
    "                key[0], train, test_shape, train_flag=1)\n",
    "            rmse[key[0]] = rmse_i\n",
    "            model_predictions[key[0]] = predictions\n",
    "\n",
    "    print(\"Moving_Average done\")\n",
    "\n",
    "    return rmse, model_predictions\n",
    "\n",
    "def model_MA(key, train, test_shape, train_flag=0):\n",
    "    forecast = []\n",
    "    rmse_val = []\n",
    "    try:\n",
    "        train = train.values\n",
    "    except:\n",
    "        train = train\n",
    "    history = [np.asscalar(x) for x in train]\n",
    "\n",
    "# TRAIN\n",
    "    if train_flag == 1:\n",
    "        itr = 5\n",
    "        data = pd.DataFrame(history)\n",
    "        for i in range(3):\n",
    "            pred_temp = []\n",
    "            v_train = data[:-itr]\n",
    "            v_train = v_train.values\n",
    "            v_expected = data.tail(itr).head(3)\n",
    "            for j in range(3):\n",
    "                if key == 'sma':\n",
    "                    pred1 = np.mean(v_train[-3:])\n",
    "                    pred_temp.append(pred1)\n",
    "                    v_train = np.append(v_train, pred1)\n",
    "                if key == 'wma':\n",
    "                    alpha = [0.25, 0.35, 0.4]\n",
    "                    pred1 = v_train[-3:]\n",
    "                    pred1 = [np.asscalar(x) for x in pred1]\n",
    "                    pred1 = np.dot(pred1, alpha)\n",
    "                    pred_temp.append(pred1)\n",
    "                    v_train = np.append(v_train, pred1)\n",
    "            rmse_val.append(calculate_rmse(key, v_expected, pred_temp))\n",
    "\n",
    "            if i == 2:\n",
    "                forecast.extend(pred_temp)\n",
    "            else:\n",
    "                forecast.append(pred_temp[0])\n",
    "\n",
    "            itr = itr-1\n",
    "            #   FORECAST\n",
    "    else:\n",
    "\n",
    "        for num in range(test_shape):\n",
    "            if key == 'sma':\n",
    "                test_new = pd.DataFrame(history)\n",
    "                pred1 = (test_new.tail(3).mean())\n",
    "                pred1 = pred1[0]\n",
    "                forecast.append(pred1)\n",
    "                history.append(pred1)\n",
    "            if key == 'wma':\n",
    "                alpha = [0.25, 0.35, 0.4]\n",
    "                test_new = pd.DataFrame(history)\n",
    "                pred1 = test_new.tail(3)\n",
    "                pred1 = np.dot(pred1[0], alpha)\n",
    "                forecast.append(pred1)\n",
    "                history.append(pred1)\n",
    "\n",
    "    forecast = [int(i) for i in forecast]\n",
    "    return forecast, rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "NdJTNiFKXFPK"
   },
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "\n",
    "def moving_average(test, n, n1):\n",
    "    train = []\n",
    "    train = [x for x in test]\n",
    "    pred = []\n",
    "    for num in range(n):\n",
    "        test_new = pd.DataFrame(train)\n",
    "        pred1 = (test_new.tail(n1).mean())\n",
    "        pred1 = pred1[0]\n",
    "        pred.append(pred1)\n",
    "        train.append(pred1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "8h8QU-bBXIhP"
   },
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "\n",
    "def calculate_rmse(key, expected, predictions):\n",
    "    # check the RMSE formula. Replace with the Sklearn Formula\n",
    "    expected = np.array(expected)\n",
    "    rmse = sqrt(mean_squared_error(expected, predictions))\n",
    "    print(\"RMSE FOR %s: %d \" % (key, rmse))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "M-7AderfXK6L"
   },
   "outputs": [],
   "source": [
    "# In[14]:\n",
    "\n",
    "def data_imputation(dataset, freq):\n",
    "    # Taking the mean of nearest neighbours to fill NA\n",
    "\n",
    "    data_forward = dataset.fillna(method='ffill')\n",
    "    data_back = dataset.fillna(method='bfill')\n",
    "    data_back.fillna(0, inplace=True)\n",
    "    data_forward.fillna(0, inplace=True)\n",
    "\n",
    "    new_data = (data_forward.values + data_back.values) / 2\n",
    "    dataset = pd.DataFrame(dataset.values)\n",
    "\n",
    "#    dataset = dataset['sales']\n",
    "    imput = dataset.isnull()\n",
    "    imput = imput[0]\n",
    "    dataset = dataset[0]\n",
    "    for i in range(len(dataset)):\n",
    "\n",
    "        div_factor = 3\n",
    "        if imput[i] == True:\n",
    "            # Negative index, set previous as 0\n",
    "            #            print(\"NULL\")\n",
    "            if i - freq < 0:\n",
    "                prev_value = 0\n",
    "                div_factor -= 1\n",
    "\n",
    "            else:\n",
    "                prev_value = dataset[i - freq]\n",
    "\n",
    "            # Outside boundary or next value is NaN, set previous as 0\n",
    "            if i + freq >= len(dataset) or imput[i + freq] == True:\n",
    "  \n",
    "                next_value = 0\n",
    "                div_factor -= 1\n",
    "\n",
    "            # Fetch next value\n",
    "            else:\n",
    "          \n",
    "                next_value = dataset[i + freq]\n",
    "\n",
    "            dataset[i] = (new_data[i] + prev_value + next_value)/div_factor\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "\n",
    "#    print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "XDkGjmDcXTTm"
   },
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "\n",
    "def weighted_moving_average(test1, n, n1):\n",
    "    #    print(\"Weighted moving average\")\n",
    "    alpha = [0.25, 0.3, 0.45]\n",
    "    train = [x for x in test1]\n",
    "    pred = []\n",
    "    for num in range(n):\n",
    "        test_new = pd.DataFrame(train)\n",
    "        pred1 = test_new.tail(n1)\n",
    "        pred1 = np.dot(pred1[0], alpha)\n",
    "        pred.append(pred1)\n",
    "        train.append(pred1)\n",
    "    pred = [int(i) for i in pred]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "dZCB_Yf0XV5P"
   },
   "outputs": [],
   "source": [
    "# In[16]:\n",
    "\n",
    "def mean_standard_deviation(dataset):\n",
    "    mu = np.mean(dataset.values)\n",
    "    sd = np.std(dataset.values)\n",
    "\n",
    "    ub = mu + (3 * sd)\n",
    "    lb = mu - (3 * sd)\n",
    "\n",
    "    return lb, ub\n",
    "\n",
    "def median_absolute_deviation(dataset, median):\n",
    "    median_list = list()\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    for i in range(0, len(dataset)):\n",
    "        value = dataset.T[i] - median\n",
    "        median_list.append(value)\n",
    "    ms = np.abs(median_list)\n",
    "    mad = np.median(ms)\n",
    "    ub = median + (3 * mad)\n",
    "    lb = median - (3 * mad)\n",
    "\n",
    "    return ub, lb\n",
    "\n",
    "def outlier_treatment(dataset):\n",
    "    # Treated out liers from the dataset\n",
    "    median = np.median(dataset)\n",
    "    if median == 0:\n",
    "        ub, lb = mean_standard_deviation(dataset)\n",
    "    else:\n",
    "        ub, lb = median_absolute_deviation(dataset, median)\n",
    "    new_dataset = np.clip(dataset, lb, ub)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "AenH2OhjXY-I"
   },
   "outputs": [],
   "source": [
    "# In[17]:\n",
    "\n",
    "def Sesonal_detection(sku_data):\n",
    "    median = np.median(sku_data)\n",
    "\n",
    "    if median == 0:\n",
    "        ub, lb = mean_standard_deviation(sku_data)\n",
    "    else:\n",
    "        ub, lb = median_absolute_deviation(sku_data, median)\n",
    "    outliers1 = sku_data > ub\n",
    "    outliers2 = sku_data < lb\n",
    "    a = np.where(outliers1 == True)[0]\n",
    "    b = np.where(outliers2 == True)[0]\n",
    "    flag1 = flag2 = 1\n",
    "    if len(a) == 0:\n",
    "        flag1 = 0\n",
    "        remove1 = []\n",
    "    if len(b) == 0:\n",
    "        flag2 = 0\n",
    "        remove2 = []\n",
    "\n",
    "    if flag1 == 1:\n",
    "        k = np.zeros([len(a)-1, len(a)])\n",
    "        for i in range(0, (len(a)-1)):\n",
    "            for j in range(1, len(a)):\n",
    "                if a[j] == (a[i]+12) or a[j] == (a[i]+24):\n",
    "                    k[i][j] = 1\n",
    "                else:\n",
    "                    k[i][j] = 0\n",
    "        m = np.where(k != 0)\n",
    "        z = np.unique(m).tolist()\n",
    "        remove1 = a[z]\n",
    "    if flag2 == 1:\n",
    "        q = np.zeros([len(b)-1, len(b)])\n",
    "        for i in range(0, (len(b)-1)):\n",
    "            for j in range(1, len(b)):\n",
    "                if b[j] == (b[i]+12) or b[j] == (b[i]+24):\n",
    "                    q[i][j] = 1\n",
    "                else:\n",
    "                    q[i][j] = 0\n",
    "        n = np.where(q != 0)\n",
    "        z1 = np.unique(n).tolist()\n",
    "        remove2 = b[z1]\n",
    "    return remove1, remove2, flag1, flag2\n",
    "\n",
    "# dataset, actual interval in numbers, bucket size\n",
    "\n",
    "def outlier_treatment_tech(dataset, interval, pt):\n",
    "    # TODO: remove interval\n",
    "    start = 0\n",
    "    end = pt\n",
    "    sku_data = [0]*len(dataset)\n",
    "    while end < len(dataset):\n",
    "        sku_data[start:end] = outlier_treatment(dataset[start:end])\n",
    "        start = end\n",
    "        end += pt\n",
    "    if start < len(dataset):\n",
    "        sku_data[start:len(dataset)] = outlier_treatment(\n",
    "            dataset[start:end])\n",
    "    sku_data = pd.DataFrame(sku_data)\n",
    "    return sku_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "UTwTWDCpYD2A"
   },
   "outputs": [],
   "source": [
    "# In[18]:\n",
    "\n",
    "def acf_plot(dataset, freq):\n",
    "    res = acf(dataset)\n",
    "    # plot_acf(dataset)\n",
    "    acfval = [0]*len(res)\n",
    "    ub = 1.96/np.sqrt(len(dataset))\n",
    "    p1 = 1\n",
    "    for i in range(len(res)):\n",
    "        acfval[i] = abs(res[i])\n",
    "    acfval.sort(reverse=True)\n",
    "    acfval = np.array(acfval[1:])\n",
    "    pshort = np.array(acfval[0:3])\n",
    "    pshortind = [0]*len(pshort)\n",
    "    for i in range(len(pshort)):\n",
    "        pshortind[i] = np.where(abs(res) == pshort[i])[0][0]\n",
    "    ind = np.where(acfval > ub)[0]\n",
    "    finalacf = acfval[ind]\n",
    "    plist = [0]*len(finalacf)\n",
    "    for i in range(len(finalacf)):\n",
    "        plist[i] = np.where(abs(res) == finalacf[i])[0][0]\n",
    "\n",
    "    while len(finalacf) > 0:\n",
    "        p1 = np.where(abs(res) == max(finalacf))[0][0]\n",
    "        if p1 > freq:\n",
    "            finalacf = finalacf[1:]\n",
    "        else:\n",
    "            break\n",
    "    return p1, pshortind, plist\n",
    "\n",
    "def pacf_plot(dataset, freq):\n",
    "    res = pacf(dataset)\n",
    "    # plot_pacf(dataset)\n",
    "    pacfval = [0]*len(res)\n",
    "    ub = 1.96/np.sqrt(len(dataset))\n",
    "    q1 = 0\n",
    "    for i in range(len(res)):\n",
    "        pacfval[i] = abs(res[i])\n",
    "    pacfval.sort(reverse=True)\n",
    "    pacfval = np.array(pacfval[1:])\n",
    "    ind = np.where(pacfval > ub)[0]\n",
    "    finalpacf = pacfval[ind]\n",
    "    while len(finalpacf) > 0:\n",
    "        q1 = np.where(abs(res) == max(finalpacf))[0]\n",
    "        q1 = q1[0]\n",
    "        if q1 > int(freq/2):\n",
    "            finalpacf = finalpacf[1:]\n",
    "        else:\n",
    "            break\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ObseDjqcYIES"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dickeyfullertest(series):\n",
    "    dftest = adfuller(series, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=[\n",
    "                            'Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "\n",
    "    if dfoutput['p-value'] > 0.05:\n",
    "        return 0  # not stationary\n",
    "\n",
    "    else:\n",
    "        return 1  # stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "P1jmOGOrYLm3"
   },
   "outputs": [],
   "source": [
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    if(interval != 0):\n",
    "        for i in range(interval, len(dataset)):\n",
    "            value = dataset[i] - dataset[i - interval]\n",
    "            diff.append(value)\n",
    "    else:\n",
    "        diff = list(dataset)\n",
    "\n",
    "    return pd.Series(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "GQE640MBYMIN"
   },
   "outputs": [],
   "source": [
    "def timeseries_to_supervised(dataset, lag=1):\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    y = [dataset.shift(i) for i in range(1, lag + 1)]\n",
    "    y.append(dataset)\n",
    "    dataset = pd.concat(y, axis=1)\n",
    "    cols = []\n",
    "    for i in range(lag):\n",
    "        cols.append('x_' + str(i))\n",
    "    cols.append('y')\n",
    "    dataset.columns = cols\n",
    "    dataset.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def scaler_selection(key):\n",
    "    if key == 'lr' or key == 'lasso' or key == 'ridge' or key == 'knn':\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "    elif key == 'svmr':\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "tyY5Qzm-YOl-"
   },
   "outputs": [],
   "source": [
    "def fit_model(train_data, model):\n",
    "    X, y = train_data[:, 0:-1], train_data[:, -1]\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def forecast_model(model, X):\n",
    "\n",
    "    yhat = model.predict(X)\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "vgpYQr7bYRMz"
   },
   "outputs": [],
   "source": [
    "def model_Naive(key, train, test_shape, order, rept, train_flag=0):\n",
    "    forecast = []\n",
    "    p = order[0]\n",
    "    rmse_val = []\n",
    "    try:\n",
    "        train = train.values\n",
    "    except:\n",
    "        train = train\n",
    "\n",
    "    history = [np.asscalar(x) for x in train]\n",
    "    if train_flag == 1:\n",
    "        itr = 5\n",
    "        data = pd.DataFrame(history)\n",
    "        for i in range(3):\n",
    "            pred_temp = []\n",
    "            v_train = data[:-itr]\n",
    "            v_train = v_train.values\n",
    "            v_expected = data.tail(itr).head(3)\n",
    "\n",
    "            for j in range(3):\n",
    "\n",
    "                if key == 'naive':\n",
    "                    try:\n",
    "                        t = v_train[-p]\n",
    "                    except:\n",
    "                        t = 0\n",
    "\n",
    "                    pred_temp.append(t)\n",
    "                    v_train = np.append(v_train, t)\n",
    "                elif key == 'naive_rept':\n",
    "                    try:\n",
    "                        t = v_train[-rept]\n",
    "                    except:\n",
    "                        t = 0\n",
    "                    pred_temp.append(t)\n",
    "                    v_train = np.append(v_train, t)\n",
    "                elif key == 'naive3':\n",
    "                    try:\n",
    "                        t = v_train[-3]\n",
    "                    except:\n",
    "                        t = 0\n",
    "                    pred_temp.append(t)\n",
    "                    v_train = np.append(v_train, t)\n",
    "                elif key == 'naive6':\n",
    "                    try:\n",
    "                        t = v_train[-6]\n",
    "                    except:\n",
    "                        t = 0\n",
    "                    pred_temp.append(t)\n",
    "                    v_train = np.append(v_train, t)\n",
    "                elif key == 'naive12':\n",
    "                    try:\n",
    "                        t = v_train[-12]\n",
    "                    except:\n",
    "                        t = 0\n",
    "                    pred_temp.append(t)\n",
    "                    v_train = np.append(v_train, t)\n",
    "                elif key == 'naive12wa':\n",
    "                    try:\n",
    "                        yt = v_train[-12]\n",
    "                    except:\n",
    "                        yt = 0\n",
    "                    try:\n",
    "                        yt_1 = v_train[-24]\n",
    "                    except:\n",
    "                        yt_1 = 0\n",
    "                    t = ((0.55*yt)+(0.45*yt_1))\n",
    "                    pred_temp.append(t)\n",
    "                    v_train = np.append(v_train, t)\n",
    "                elif key == 'naive6wa':\n",
    "                    try:\n",
    "                        # naive of six\n",
    "                        try:\n",
    "                            naive_six = v_train[-6]\n",
    "                        except:\n",
    "                            naive_six = 0\n",
    "                        # weighted moving average\n",
    "                        alpha = [0.25, 0.35, 0.4]\n",
    "                        pred1 = v_train[-3:]\n",
    "                        pred1 = [np.asscalar(x) for x in pred1]\n",
    "                        weighted_avg = np.dot(pred1, alpha)\n",
    "                        # ensemble\n",
    "                        t = (0.75*naive_six)+(0.25*weighted_avg)\n",
    "                    except:\n",
    "                        t = 0\n",
    "                    pred_temp.append(t)\n",
    "                    v_train = np.append(v_train, t)\n",
    "\n",
    "            rmse_val.append(calculate_rmse(key, v_expected, pred_temp))\n",
    "\n",
    "            if i == 2:\n",
    "                forecast.extend(pred_temp)\n",
    "            else:\n",
    "                forecast.append(pred_temp[0])\n",
    "\n",
    "            itr = itr-1\n",
    "    else:\n",
    "        for num in range(test_shape):\n",
    "            if key == 'naive':\n",
    "                try:\n",
    "                    t = history[-p]\n",
    "                    forecast.append(t)\n",
    "                    history.append(t)\n",
    "                except:\n",
    "                    pass\n",
    "            elif key == 'naive2':\n",
    "                try:\n",
    "                    t = history[-rept]\n",
    "                    forecast.append(t)\n",
    "                    history.append(t)\n",
    "                except:\n",
    "                    pass\n",
    "            elif key == 'naive3':\n",
    "                try:\n",
    "                    t = history[-3]\n",
    "                    forecast.append(t)\n",
    "                    history.append(t)\n",
    "                except:\n",
    "                    pass\n",
    "            elif key == 'naive6':\n",
    "                try:\n",
    "                    t = history[-6]\n",
    "                    forecast.append(t)\n",
    "                    history.append(t)\n",
    "                except:\n",
    "                    pass\n",
    "            elif key == 'naive12':\n",
    "                try:\n",
    "                    t = history[-12]\n",
    "                    forecast.append(t)\n",
    "                    history.append(t)\n",
    "                except:\n",
    "                    pass\n",
    "            elif key == 'naive12wa':\n",
    "                try:\n",
    "                    yt = history[-12]\n",
    "                except:\n",
    "                    yt = 0\n",
    "                try:\n",
    "                    yt_1 = history[-24]\n",
    "                except:\n",
    "                    yt_1 = 0\n",
    "                t = ((0.55*yt)+(0.45*yt_1))\n",
    "                forecast.append(t)\n",
    "                history.append(t)\n",
    "            elif key == 'naive6wa':\n",
    "                # naive of six\n",
    "                try:\n",
    "                    naive_six = history[-6]\n",
    "                except:\n",
    "                    naive_six = 0\n",
    "                    # weighted moving average\n",
    "                alpha = [0.25, 0.35, 0.4]\n",
    "                pred1 = history[-3:]\n",
    "        #               pred1=[np.asscalar(x) for x in pred1]\n",
    "                weighted_avg = np.dot(pred1, alpha)\n",
    "                # ensemble\n",
    "                t = (0.75*naive_six)+(0.25*weighted_avg)\n",
    "                forecast.append(t)\n",
    "                history.append(t)\n",
    "\n",
    "    forecast = [int(i) for i in forecast]\n",
    "    return forecast, rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "KJfFQ5PtYZTc"
   },
   "outputs": [],
   "source": [
    "# In[24]:\n",
    "\n",
    "def naive_forecast(dataset, freq, p, tsize):\n",
    "    rmse = dict()\n",
    "    model_predictions = dict()\n",
    "    train, test = dataset[0:-tsize], dataset[-tsize:]\n",
    "    test = pd.DataFrame(test)\n",
    "    test = test.reset_index(drop=True)\n",
    "#    expected = test\n",
    "    test_shape = len(test)\n",
    "    sam = np.array(dataset)\n",
    "    repeat = check_repetition(sam, freq, 1, len(sam))\n",
    "\n",
    "    key = 'naive'\n",
    "    predictions, rmse_i = model_Naive(\n",
    "        key, train, test_shape, p, repeat, train_flag=1)\n",
    "    rmse[key] = rmse_i\n",
    "    model_predictions[key] = predictions\n",
    "\n",
    "    key = 'naive_rept'\n",
    "    predictions, rmse_i = model_Naive(\n",
    "        key, train, test_shape, p, repeat, train_flag=1)\n",
    "    rmse[key] = rmse_i\n",
    "    model_predictions[key] = predictions\n",
    "\n",
    "    key = 'naive3'\n",
    "    predictions, rmse_i = model_Naive(\n",
    "        key, train, test_shape, p, repeat, train_flag=1)\n",
    "    rmse[key] = rmse_i\n",
    "    model_predictions[key] = predictions\n",
    "\n",
    "    key = 'naive6'\n",
    "    predictions, rmse_i = model_Naive(\n",
    "        key, train, test_shape, p, repeat, train_flag=1)\n",
    "    rmse[key] = rmse_i\n",
    "    model_predictions[key] = predictions\n",
    "\n",
    "    key = 'naive12'\n",
    "    predictions, rmse_i = model_Naive(\n",
    "        key, train, test_shape, p, repeat, train_flag=1)\n",
    "    rmse[key] = rmse_i\n",
    "    model_predictions[key] = predictions\n",
    "\n",
    "    key = 'naive12wa'\n",
    "    predictions, rmse_i = model_Naive(\n",
    "        key, train, test_shape, p, repeat, train_flag=1)\n",
    "    rmse[key] = rmse_i\n",
    "    model_predictions[key] = predictions\n",
    "\n",
    "    return rmse, model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "jGH0dx1hYcuT"
   },
   "outputs": [],
   "source": [
    "# In[25]:\n",
    "\n",
    "def check_repetition(arr, limit, index_start, index_end):\n",
    "    length = index_start\n",
    "    try:\n",
    "        for i in range(0, int(len(arr)/length)):\n",
    "            condition = np.array(\n",
    "                arr[i:int(i+length)]) - np.array(arr[int(i+length):int(i+2*length)])\n",
    "            condition = np.sum([abs(number) for number in condition])\n",
    "            if condition >= limit:\n",
    "                if length + 1 <= index_end:\n",
    "                    return check_repetition(arr, limit, length + 1, index_end)\n",
    "            # if not than no more computations needed\n",
    "                else:\n",
    "                    return 0\n",
    "\n",
    "            if i == int(len(arr)/length)-2:\n",
    "                return(length)\n",
    "    except:\n",
    "        for i in range(0, int(len(arr)/length)):\n",
    "            if i+2*length+1 <= index_end and i+length+1 <= index_end:\n",
    "                break\n",
    "            condition = np.array(\n",
    "                arr[i:int(i+length)]) - np.array(arr[int(i+length):int(i+2*length)])\n",
    "            condition = np.sum([abs(number) for number in condition])\n",
    "            if condition >= limit:\n",
    "                if length + 1 <= index_end:\n",
    "                    return check_repetition(arr, limit, length + 1, index_end)\n",
    "            # if not than no more computations needed\n",
    "                else:\n",
    "                    return 0\n",
    "\n",
    "            if i == int(len(arr)/length)-2:\n",
    "                return(length)\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "ttcC3yJmYhNz"
   },
   "outputs": [],
   "source": [
    "# In[26]:\n",
    "\n",
    "def model_ML(dataset=[], tsize=0, test_shape=0, model=np.nan, key='', order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    pred_temp = []\n",
    "    rmse_val = []\n",
    "    scale_flag = 0\n",
    "    if key == 'lr' or key == 'lasso' or key == 'ridge' or key == 'knn' or key == 'svmr':\n",
    "        scale_flag = 1\n",
    "\n",
    "    if train_flag == 1:\n",
    "        itr = forecast_period  # changed\n",
    "        #print(3)\n",
    "        for i in range(3):\n",
    "            expected = pd.DataFrame(dataset)\n",
    "            expected = expected.tail(itr).head(3)\n",
    "            expected = expected.reset_index(drop=True)\n",
    "            # print(dataset[:-itr])\n",
    "            train = dataset[:-itr]\n",
    "\n",
    "            diff_values = difference(train, order[1])\n",
    "\n",
    "            if scale_flag == 1:\n",
    "                scaler = scaler_selection(key)\n",
    "                diff_values = scaler.fit_transform(\n",
    "                    pd.DataFrame(diff_values).values.reshape(-1, 1))\n",
    "          \n",
    "            supervised = timeseries_to_supervised(train, order[0])\n",
    "\n",
    "            data = supervised.values\n",
    "            #print(\"fit\")\n",
    "            RF_model = fit_model(data, model)\n",
    "            pred_temp = []\n",
    "            #print(\"data\")\n",
    "            for j in range(test_shape):\n",
    "                X = data[:, 0:-1]\n",
    "                yhat = forecast_model(RF_model, X)\n",
    "                #print(7)\n",
    "# TODO: Inverse differencing and scaling\n",
    "\n",
    "                forecast = yhat[-1]\n",
    "                if forecast <= 0:\n",
    "                    forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "\n",
    "                pred_temp.append(forecast)\n",
    "              \n",
    "                train = np.append(train, forecast)\n",
    "\n",
    "                diff_train = difference(train, order[1])\n",
    "\n",
    "                if scale_flag == 1:\n",
    "                    scaler = scaler_selection(key)\n",
    "                    diff_train = scaler.fit_transform(\n",
    "                        pd.DataFrame(diff_train).values.reshape(-1, 1))\n",
    "\n",
    "                supervised = timeseries_to_supervised(train, order[0])\n",
    "                data = supervised.values\n",
    "\n",
    "            pred_temp = pred_temp[1:4]\n",
    "\n",
    "            if i == 2:\n",
    "                predictions.extend(pred_temp)\n",
    "            else:\n",
    "                predictions.append(pred_temp[0])\n",
    "\n",
    "            rmse_val.append(calculate_rmse(key, expected, pred_temp))\n",
    "            itr = itr-1\n",
    "         \n",
    "    else:\n",
    "\n",
    "        dataset_1 = copy.deepcopy(dataset)\n",
    "        diff_values = difference(dataset_1, order[1])\n",
    "       \n",
    "        if scale_flag == 1:\n",
    "            scaler = scaler_selection(key)\n",
    "            diff_values = scaler.fit_transform(\n",
    "                pd.DataFrame(diff_values).values.reshape(-1, 1))\n",
    "\n",
    "        supervised = timeseries_to_supervised(diff_values, order[0])\n",
    "        data = supervised.values\n",
    "\n",
    "        RF_model = fit_model(data, model)\n",
    "\n",
    "        for i in range(test_shape):\n",
    "\n",
    "            X = data[:, 0:-1]\n",
    "\n",
    "            yhat = forecast_model(RF_model, X)\n",
    "#\n",
    "\n",
    "            forecast = yhat[-1]\n",
    "            if forecast <= 0:\n",
    "                forecast = weighted_moving_average(data, 1, 3)[0]\n",
    "\n",
    "            predictions.append(forecast)\n",
    "            dataset_1 = np.append(dataset_1, forecast)\n",
    "\n",
    "            diff_values = difference(dataset_1, order[1])\n",
    "\n",
    "            if scale_flag == 1:\n",
    "                scaler = scaler_selection(key)\n",
    "                diff_values = scaler.fit_transform(\n",
    "                    pd.DataFrame(diff_values).values.reshape(-1, 1))\n",
    "\n",
    "            supervised = timeseries_to_supervised(diff_values, order[0])\n",
    "            data = supervised.values\n",
    "\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions, rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "X43-UIKaYnrf"
   },
   "outputs": [],
   "source": [
    "# In[27]:\n",
    "\n",
    "def models_ML():\n",
    "    models = dict()\n",
    "    n_trees = 100\n",
    "    # prameters for RandomSearch\n",
    "    lr_param = {\"fit_intercept\": [True, False], \"normalize\": [\n",
    "        False], \"copy_X\": [True, False]}\n",
    "#    knn_param = {\"n_neighbors\":[2,3,4,5,6,7,8],\"metric\": [\"euclidean\", \"cityblock\"]}\n",
    "    dtree_param = {\"max_depth\": [3, None], \"min_samples_leaf\": sp_randint(1, 11), \"criterion\": [\n",
    "        \"mse\"], \"splitter\": [\"best\", \"random\"], \"max_features\": [\"auto\", \"sqrt\", None]}\n",
    "    lasso_param = {\"alpha\": [0.02, 0.024, 0.025, 0.026, 0.03], \"fit_intercept\": [\n",
    "        True, False], \"normalize\": [True, False], \"selection\": [\"random\"]}\n",
    "    ridge_param = {\"alpha\": [200, 230, 250, 265, 270, 275, 290, 300, 500], \"fit_intercept\": [\n",
    "        True, False], \"normalize\": [True, False], \"solver\": [\"auto\"]}\n",
    "    elas_param = {\"alpha\": list(np.logspace(-5, 2, 8)), \"l1_ratio\": [.2, .4, .6, .8], \"fit_intercept\": [\n",
    "        True, False], \"normalize\": [True, False], \"precompute\": [True, False]}\n",
    "\n",
    "\n",
    "    models['lr'] = RandomizedSearchCV(\n",
    "        LinearRegression(), lr_param, n_jobs=1, random_state=42)\n",
    "    models['lasso'] = RandomizedSearchCV(\n",
    "        Lasso(), lasso_param, n_jobs=1, n_iter=100, random_state=42)\n",
    "    models['ridge'] = RandomizedSearchCV(\n",
    "        Ridge(), ridge_param, n_jobs=1, n_iter=100, random_state=42)\n",
    "    models['en'] = RandomizedSearchCV(ElasticNet(\n",
    "    ), elas_param, scoring='neg_mean_squared_error', n_jobs=1, n_iter=100, cv=10, random_state=42)\n",
    "    #models['huber']             = HuberRegressor()\n",
    "    models['llars'] = LassoLars()\n",
    "    models['pa'] = PassiveAggressiveRegressor(\n",
    "        max_iter=1000, tol=1e-3, random_state=42)\n",
    "    models['knn'] = KNeighborsRegressor(n_neighbors=3)\n",
    "    models['cart'] = RandomizedSearchCV(\n",
    "        DecisionTreeRegressor(), dtree_param, n_jobs=1, n_iter=100, random_state=42)\n",
    "    models['extra'] = ExtraTreeRegressor(random_state=42)\n",
    "    models['svmr'] = SVR()\n",
    "\n",
    "    n_trees = 100\n",
    "    models['ada'] = AdaBoostRegressor(\n",
    "        n_estimators=n_trees, random_state=42)\n",
    "    models['bag'] = BaggingRegressor(n_estimators=n_trees)\n",
    "    models['rf'] = RandomForestRegressor(\n",
    "        n_estimators=n_trees, random_state=42)\n",
    "    \n",
    "    models['gbm'] = GradientBoostingRegressor(\n",
    "        n_estimators=n_trees, random_state=42)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "w-eT9PNpYyC4"
   },
   "outputs": [],
   "source": [
    "# In[28]:\n",
    "\n",
    "def init_test_shape():\n",
    "    test_shape_incr = dict()\n",
    "    test_shape_incr['svmr'] = 0\n",
    "    test_shape_incr['ada'] = 0\n",
    "    test_shape_incr['lr'] = 0\n",
    "    test_shape_incr['lasso'] = 0\n",
    "    test_shape_incr['ridge'] = 0  # 1\n",
    "    test_shape_incr['en'] = 0\n",
    "    # test_shape_incr['huber']    = 0#1\n",
    "    test_shape_incr['llars'] = 0\n",
    "    test_shape_incr['pa'] = 0\n",
    "    test_shape_incr['knn'] = 0\n",
    "    test_shape_incr['cart'] = 0\n",
    "    test_shape_incr['extra'] = 3\n",
    "    test_shape_incr['bag'] = 3  # 1\n",
    "    test_shape_incr['rf'] = 3  # 1\n",
    "    test_shape_incr['et'] = 3  # 1\n",
    "    test_shape_incr['gbm'] = 3  # 1#\n",
    "\n",
    "    return test_shape_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "x_JgGvp0Y-9D"
   },
   "outputs": [],
   "source": [
    "# In[29]:\n",
    "\n",
    "def model_ES(key, train, test_shape=0, train_flag=0, test=[]):\n",
    "    predictions = []\n",
    "    rmse_val = []\n",
    "\n",
    "    try:\n",
    "        train = train.values\n",
    "    except:\n",
    "        train = train\n",
    "    history = [np.asscalar(x) for x in train]\n",
    "\n",
    "#   TRAIN\n",
    "    if train_flag == 1:\n",
    "        itr = 5\n",
    "        data = pd.DataFrame(history)\n",
    "        for i in range(3):\n",
    "            pred_temp = []\n",
    "            v_train = [np.asscalar(x) for x in data[:-itr].values]\n",
    "            v_expected = data.tail(itr).head(3).reset_index(drop=True)\n",
    "            try:\n",
    "                for t in range(3):\n",
    "                    if key == 'SES':\n",
    "                        model = SimpleExpSmoothing(history)\n",
    "                    elif key == 'HWES':\n",
    "                        model = ExponentialSmoothing(history)\n",
    "                    model_fit = model.fit()\n",
    "                    yhat = model_fit.predict(len(history), len(history))\n",
    "                    if yhat < 0:\n",
    "                        yhat = weighted_moving_average(history, 1, 3)\n",
    "                    yhat = yhat[0]\n",
    "                    pred_temp.append(yhat)\n",
    "                    v_train.append(yhat)\n",
    "            except:\n",
    "                pred_temp.extend(moving_average(\n",
    "                    v_train, 3 - len(pred_temp), 3))\n",
    "            rmse_val.append(calculate_rmse(key, v_expected, pred_temp))\n",
    "\n",
    "            if i == 2:\n",
    "                predictions.extend(pred_temp)\n",
    "            else:\n",
    "                predictions.append(pred_temp[0])\n",
    "\n",
    "            itr = itr-1\n",
    "#   FORECAST\n",
    "    else:\n",
    "        try:\n",
    "            for t in range(test_shape):\n",
    "                if key == 'SES':\n",
    "                    model = SimpleExpSmoothing(history)\n",
    "                elif key == 'HWES':\n",
    "                    model = ExponentialSmoothing(history)\n",
    "                model_fit = model.fit()\n",
    "                yhat = model_fit.predict(len(history), len(history))\n",
    "                if yhat < 0:\n",
    "                    yhat = weighted_moving_average(history, 1, 3)\n",
    "                yhat = yhat[0]\n",
    "                predictions.append(yhat)\n",
    "                history.append(yhat)\n",
    "        except:\n",
    "            predictions.extend(moving_average(\n",
    "                history, test_shape - len(predictions), 3))\n",
    "\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions, rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "mv0Ax0pIZAbq"
   },
   "outputs": [],
   "source": [
    "# In[30]:\n",
    "\n",
    "def model_ARIMA(key, train, test_shape, order, train_flag=0, test=[]):\n",
    "    predictions = []\n",
    "    rmse_val = []\n",
    "    if(train_flag):\n",
    "        test = test[0]\n",
    "    try:\n",
    "        train = train.values\n",
    "    except:\n",
    "        train = train\n",
    "    history = [np.asscalar(x) for x in train]\n",
    "\n",
    "    if train_flag == 1:\n",
    "        itr = 5\n",
    "        data = pd.DataFrame(history)\n",
    "        for i in range(3):\n",
    "            pred_temp = []\n",
    "            v_train = [np.asscalar(x) for x in data[:-itr].values]\n",
    "            v_expected = data.tail(itr).head(3).reset_index(drop=True)\n",
    "            try:\n",
    "                if key not in [\"spl_ARIMA\",\"STL_ARIMA\"]:\n",
    "                    order = (order[0], 1, order[2])\n",
    "                    for j in range(3):\n",
    "                        model = ARIMA(v_train, order=order)\n",
    "                        model_fit = model.fit(disp=0)\n",
    "                        yhat = model_fit.forecast()[0]\n",
    "                        if yhat < 0 or yhat==np.nan:\n",
    "                            yhat = weighted_moving_average(history, 1, 3)\n",
    "                            yhat = yhat[0]\n",
    "                        pred_temp.append(yhat)\n",
    "                        v_train.append(yhat)\n",
    "                else:\n",
    "                    if key not in [\"STL_ARIMA\",\"ARIMA\"]:\n",
    "                        model = ARIMA(train, order=order)\n",
    "                        model_fit = model.fit(disp=0)\n",
    "                        pred_temp = model_fit.forecast(3)[0].tolist()\n",
    "                    else:\n",
    "                        model = STLForecaster(sp=12,forecaster_trend=AutoARIMA(sp=12))\n",
    "                        model.fit(train)\n",
    "                        pred_temp = model.predict(fh=list(range(1,4))).squeeze().tolist()\n",
    "                    if any(pd.Series(pred_temp).isna()):\n",
    "                            pred_temp = []\n",
    "                            pred_temp.extend(moving_average(\n",
    "                    v_train, 3 - len(pred_temp), 3))\n",
    "\n",
    "\n",
    "\n",
    "            except:\n",
    "                pred_temp.extend(moving_average(\n",
    "                    v_train, 3 - len(pred_temp), 3))\n",
    "         \n",
    "            rmse_val.append(calculate_rmse(key, v_expected, pred_temp))\n",
    "\n",
    "            if i == 2:\n",
    "                predictions.extend(pred_temp)\n",
    "            else:\n",
    "                predictions.append(pred_temp[0])\n",
    "\n",
    "            itr = itr-1\n",
    "    else:\n",
    "        try:\n",
    "     \n",
    "            if key not in [\"spl_ARIMA\",\"STL_ARIMA\"]:\n",
    "                order = (order[0], 1, order[2])\n",
    "                for t in range(test_shape):\n",
    "                    model = ARIMA(history, order=order)\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                    yhat = model_fit.forecast()[0]\n",
    "                    if yhat < 0:\n",
    "                        yhat = weighted_moving_average(history, 1, 3)\n",
    "                    yhat = yhat[0]\n",
    "                    predictions.append(yhat)\n",
    "                    history.append(yhat)\n",
    "            else:\n",
    "                if key not in [\"STL_ARIMA\",\"ARIMA\"]:\n",
    "                    model = ARIMA(history, order=order)\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                    predictions = model_fit.forecast(test_shape)[0].tolist()\n",
    "                else:\n",
    "                    model = STLForecaster(sp=12,forecaster_trend=AutoARIMA(sp=12))\n",
    "                    model.fit(train)\n",
    "                    predictions = model.predict(fh=list(range(1,test_shape+1))).squeeze().tolist()\n",
    "                if any(pd.Series(pred_temp).isna()):\n",
    "                        predictions = []\n",
    "                        predictions.extend(moving_average(\n",
    "                train,  test_shape - len(predictions), 3))\n",
    "\n",
    "        except:\n",
    "            predictions.extend(moving_average(\n",
    "                history,  test_shape - len(predictions), 3))\n",
    "\n",
    "    predictions = [0 if pd.isnull(i) else int(i) for i in predictions]\n",
    "    return predictions, rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "h4Qn3xtCZFyV"
   },
   "outputs": [],
   "source": [
    "# In[31]:\n",
    "\n",
    "def run_es_models(train, test):\n",
    "    key = ['SES', 'HWES']\n",
    "    rmse = dict()\n",
    "    test_shape = len(test)\n",
    "\n",
    "    model_predictions = dict()\n",
    "\n",
    "    if len(key) > 0:\n",
    "        if len(key) == 2:\n",
    "            # SES\n",
    "            predictions, rmse_i = model_ES(\n",
    "                key[0], train, test_shape, train_flag=1, test=test)\n",
    "            rmse[key[0]] = rmse_i\n",
    "            model_predictions[key[0]] = predictions\n",
    "            # HWES\n",
    "            predictions, rmse_i = model_ES(\n",
    "                key[1], train, test_shape, train_flag=1, test=test)\n",
    "            rmse[key[1]] = rmse_i\n",
    "            model_predictions[key[1]] = predictions\n",
    "\n",
    "        else:\n",
    "            predictions, rmse_i = model_ES(\n",
    "                key[0], train, test_shape, train_flag=1, test=test)\n",
    "            rmse[key[0]] = rmse_i\n",
    "            model_predictions[key[0]] = predictions\n",
    "\n",
    "            model_predictions[key[0]] = predictions\n",
    "\n",
    "    return rmse, model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "LYvstiPBZIiP"
   },
   "outputs": [],
   "source": [
    "# In[32]:\n",
    "\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    "\n",
    "def model_ARMA(key, train, test_shape, order, train_flag=0, test=[]):\n",
    "    predictions = []\n",
    "    rmse_val = []\n",
    "    if(train_flag):\n",
    "        test = test[0]\n",
    "    try:\n",
    "        train = train.values\n",
    "    except:\n",
    "        train = train\n",
    "    history = [np.asscalar(x) for x in train]\n",
    "\n",
    "    if train_flag == 1:\n",
    "        itr = 5\n",
    "        data = pd.DataFrame(history)\n",
    "        for i in range(3):\n",
    "            pred_temp = []\n",
    "            v_train = [np.asscalar(x) for x in data[:-itr].values]\n",
    "            v_expected = data.tail(itr).head(3).reset_index(drop=True)\n",
    "            try:\n",
    "                for j in range(3):\n",
    "                    model = ARMA(v_train, order=order)\n",
    "                    model_fit = model.fit(\n",
    "                        disp=0, transparams=False, trend='nc')\n",
    "                    yhat = model_fit.forecast()[0]\n",
    "                    pred = yhat\n",
    "                    if pred < 0:\n",
    "                        pred = weighted_moving_average(v_train, 1, 3)\n",
    "                        pred = pred[0]\n",
    "                    pred_temp.append(pred)\n",
    "                    v_train.append(pred)\n",
    "\n",
    "            except:\n",
    "                pred_temp.extend(moving_average(\n",
    "                    v_train, 3 - len(pred_temp), 3))\n",
    "            # plotting(key, pred_temp, v_expected)\n",
    "\n",
    "            if i == 2:\n",
    "                predictions.extend(pred_temp)\n",
    "            else:\n",
    "                predictions.append(pred_temp[0])\n",
    "\n",
    "            rmse_val.append(calculate_rmse(key, v_expected, pred_temp))\n",
    "            itr = itr-1\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            for t in range(test_shape):\n",
    "                model = ARMA(history, order=order)\n",
    "                model_fit = model.fit(\n",
    "                    disp=0, transparams=False, trend='nc')\n",
    "                yhat = model_fit.forecast()[0]\n",
    "                inverted = list()\n",
    "                for i in range(len(yhat)):\n",
    "                    value = inverse_difference(\n",
    "                        history, yhat[i], len(history) - i)\n",
    "                    inverted.append(value)\n",
    "                inverted = np.array(inverted)\n",
    "                pred = inverted[-1]\n",
    "                if pred < 0:\n",
    "                    pred = weighted_moving_average(history, 1, 3)\n",
    "                predictions.append(pred)\n",
    "                history.append(yhat)\n",
    "        except:\n",
    "            predictions.extend(moving_average(\n",
    "                history, test_shape - len(predictions), 3))\n",
    "\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions, rmse_val\n",
    "\n",
    "def init_ARIMA_models(order):\n",
    "    models = dict()\n",
    "    models['AR'] = (order[0], 0, 0)\n",
    "    models['MA'] = (0, 0, order[2])\n",
    "    models['ARMA'] = (order[0], order[2])\n",
    "    models['ARIMA'] = order\n",
    "    models[\"spl_ARIMA\"] = (0,2,1)\n",
    "    models[\"STL_ARIMA\"] = []\n",
    "    return models\n",
    "\n",
    "def init_ES_models():\n",
    "    models = dict()\n",
    "\n",
    "    models['SES'] = SimpleExpSmoothing()\n",
    "    models['HWES'] = ExponentialSmoothing()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "K8zTd2NsZOF_"
   },
   "outputs": [],
   "source": [
    "def time_series_using_ml(dataset, tsize, order):\n",
    "    models = models_ML()\n",
    "    rmse = dict()\n",
    "    model_predictions = dict()\n",
    "    for idx, key in enumerate(models.keys()):\n",
    "        test_shape = tsize\n",
    "\n",
    "        test_shape_incr = init_test_shape()\n",
    "        print(key)\n",
    "        if key in test_shape_incr:\n",
    "            test_shape = test_shape + test_shape_incr[key]\n",
    "        else:\n",
    "            test_shape = test_shape + 1\n",
    "\n",
    "        predictions, rmse_i = model_ML(\n",
    "            dataset.values, tsize, test_shape, models[key], key, order, 1)\n",
    "        predictions = predictions[-tsize:]\n",
    "\n",
    "        rmse[key] = rmse_i\n",
    "        model_predictions[key] = predictions\n",
    "    return model_predictions, rmse\n",
    "\n",
    "def run_arima_models(data, diff_data, best_order, tsize):\n",
    "    models = init_ARIMA_models(best_order)\n",
    "\n",
    "    rmse = dict()\n",
    "    model_predictions = dict()\n",
    "\n",
    "    train, test = data[0:-tsize], data[-tsize:]\n",
    "    test = pd.DataFrame(test)\n",
    "    test = test.reset_index(drop=True)\n",
    "    test_shape = len(test)\n",
    "    for key in models.keys():\n",
    "        print(\"KEY!\", key)\n",
    "        if key == 'ARMA' or key == 'AR' or key == 'MA':\n",
    "            predictions, rmse_i = model_ARMA(\n",
    "                key, train, test_shape, models[key], train_flag=1, test=test)\n",
    "\n",
    "        else:\n",
    "            predictions, rmse_i = model_ARIMA(\n",
    "                key, train, test_shape, models[key], train_flag=1, test=test)\n",
    "\n",
    "        rmse[key] = rmse_i\n",
    "\n",
    "        model_predictions[key] = predictions\n",
    "\n",
    "    return rmse, model_predictions\n",
    "\n",
    "def time_series_models(freq, data, diff_data, tsize, best_order):\n",
    "    rmse_ARIMA, predictions_ARIMA = run_arima_models(\n",
    "        data, diff_data, best_order, tsize)\n",
    "    rmse_ES, predictions_ES = run_es_models(data, diff_data)\n",
    "    rmse_naive, predictions_naive = naive_forecast(\n",
    "        data, freq, best_order, tsize)\n",
    "    rmse_ma, predictions_ma = Moving_Average(data, tsize)\n",
    "    return rmse_ARIMA, rmse_ES, rmse_naive, rmse_ma, predictions_ARIMA, predictions_ES, predictions_naive, predictions_ma\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "t0OZbDLvZSsV"
   },
   "outputs": [],
   "source": [
    "# In[34]:\n",
    "\n",
    "def Croston_TSB(dataset, forecast_period=1, alpha=0.4, beta=0.4):\n",
    "    rmse = dict()\n",
    "    pred_croston = dict()\n",
    "    rmse_val = []\n",
    "    d = np.array(dataset)  # Transform the input into a numpy array\n",
    "    cols = len(d)  # Historical period length\n",
    "    # Append np.nan into the demand array to cover future periods\n",
    "    d = np.append(d, [np.nan] * forecast_period)\n",
    "\n",
    "    #level (a), probability(p) and forecast (f)\n",
    "    a, p, f = np.full((3, cols + forecast_period),\n",
    "                        np.nan)  # Initialization\n",
    "    first_occurence = np.argmax(d[:cols] > 0)\n",
    "    a[0] = d[first_occurence]\n",
    "    p[0] = 1/(1 + first_occurence)\n",
    "    f[0] = p[0] * a[0]\n",
    "\n",
    "    # Create all the t+1 forecasts\n",
    "    for t in range(0, cols):\n",
    "        if d[t] > 0:\n",
    "            a[t + 1] = alpha * d[t] + (1 - alpha) * a[t]\n",
    "            p[t + 1] = beta * (1) + (1 - beta) * p[t]\n",
    "        else:\n",
    "            a[t + 1] = a[t]\n",
    "            p[t + 1] = (1 - beta) * p[t]\n",
    "        f[t + 1] = p[t + 1] * a[t + 1]\n",
    "\n",
    "    # Future Forecast\n",
    "    a[cols + 1: cols + forecast_period] = a[cols]\n",
    "    p[cols + 1: cols + forecast_period] = p[cols]\n",
    "    f[cols + 1: cols + forecast_period] = f[cols]\n",
    "\n",
    "    rmse_val.append(calculate_rmse(\n",
    "        'Croston', d[0:cols - 1], f[0:cols - 1]))\n",
    "    rmse['Croston'] = rmse_val\n",
    "    forecast = f[cols: cols + forecast_period]\n",
    "    forecast = [int(i) for i in forecast]\n",
    "    pred_croston['Croston'] = forecast\n",
    "    return rmse, pred_croston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "PUP3WnIRZbV6"
   },
   "outputs": [],
   "source": [
    "def model_LinearRegression(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    rmse_val = []\n",
    "\n",
    "    if train_flag == 1:\n",
    "        itr = 5\n",
    "        for i in range(3):\n",
    "            expected = pd.DataFrame(dataset)\n",
    "            expected = expected.tail(itr).head(3).reset_index(drop=True)\n",
    "\n",
    "            train = dataset[:-itr]\n",
    "            diff_values = difference(dataset, order[1])\n",
    "\n",
    "            scaler = scaler_selection('lr')\n",
    "            diff_values = scaler.fit_transform(\n",
    "                pd.DataFrame(diff_values).values.reshape(-1, 1))\n",
    "\n",
    "            supervised = timeseries_to_supervised(diff_values, order[0])\n",
    "            data = supervised.values\n",
    "\n",
    "            clf = LinearRegression()\n",
    "            param = {\"fit_intercept\": [True, False],\n",
    "                        \"normalize\": [False],\n",
    "                        \"copy_X\": [True, False]}\n",
    "            grid = GridSearchCV(clf, param, n_jobs=1)\n",
    "            model = fit_model(data, grid)\n",
    "\n",
    "            for j in range(tsize):\n",
    "                X = data[:, 0:-1]\n",
    "                yhat = forecast_model(model, X)\n",
    "\n",
    "                forecast = yhat[-1]\n",
    "                if forecast <= 0:\n",
    "                    forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "\n",
    "                predictions.append(forecast)\n",
    "                train = np.append(train, forecast)\n",
    "                diff_train = difference(train, order[1])\n",
    "                diff_train = scaler.fit_transform(\n",
    "                    pd.DataFrame(diff_train).values.reshape(-1, 1))\n",
    "\n",
    "                supervised = timeseries_to_supervised(train, order[0])\n",
    "                data = supervised.values\n",
    "\n",
    "            predictions = predictions[1:4]\n",
    "            rmse_val.append(calculate_rmse('GR_LR', expected, predictions))\n",
    "            itr = itr - 1\n",
    "\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions, rmse_val\n",
    "\n",
    "def model_SVR_Sigmoid(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    for i in range(tsize):\n",
    "        diff_values = difference(dataset, 1)\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        data = supervised.values\n",
    "\n",
    "        if train_flag == 1:\n",
    "            train = data[0:-tsize]\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        X, y = train[:, 0:-1].reshape(-1, 1), train[:, -1]\n",
    "\n",
    "        mod = SVR()\n",
    "        g = list(np.linspace(0.0001, 1, 25000))\n",
    "        C = [1]\n",
    "        param = {\"kernel\": [\"sigmoid\"],\n",
    "                    \"gamma\": g,\n",
    "                    \"C\": C}\n",
    "        random_search = RandomizedSearchCV(\n",
    "            mod, param, n_jobs=1, n_iter=100)\n",
    "        random_search.fit(X, y)\n",
    "        clf = SVR(kernel=random_search.best_params_[\"kernel\"], gamma=random_search.best_params_[\n",
    "                    \"gamma\"], C=random_search.best_params_[\"C\"])\n",
    "        clf.fit(X, y)\n",
    "        yhat = forecast_model(clf, X)\n",
    "\n",
    "        inverted = list()\n",
    "        for i in range(len(yhat)):\n",
    "            value = inverse_difference(dataset, yhat[i], len(dataset) - i)\n",
    "            inverted.append(value)\n",
    "        inverted = np.array(inverted)\n",
    "\n",
    "        forecast = inverted[-1]\n",
    "        if forecast <= 0:\n",
    "            forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "        predictions.append(forecast)\n",
    "        dataset = np.append(dataset, forecast)\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions\n",
    "\n",
    "def model_SVR_RBF(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    for i in range(tsize):\n",
    "        diff_values = difference(dataset, 1)\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        data = supervised.values\n",
    "\n",
    "        if train_flag == 1:\n",
    "            train = data[0:-tsize]\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        X, y = train[:, 0:-1].reshape(-1, 1), train[:, -1]\n",
    "\n",
    "        mod = SVR()\n",
    "\n",
    "        g = [pow(2, -15), pow(2, -14), pow(2, -13), pow(2, -12), pow(2, -11), pow(2, -10), pow(2, -9), pow(2, -8), pow(2, -7),\n",
    "                pow(2, -6), pow(2, -5), pow(2, -4), pow(2, -3), pow(2, -2), pow(2, -1), pow(1, 0), pow(2, 1), pow(2, 2), pow(2, 3)]\n",
    "\n",
    "        C = [pow(2, -5), pow(2, -4), pow(2, -3), pow(2, -2), pow(2, -1), pow(1, 0), pow(2, 1), pow(2, 2), pow(2, 3), pow(2, 4), pow(\n",
    "            2, 5), pow(2, 6), pow(2, 7), pow(2, 8), pow(2, 9), pow(2, 10), pow(2, 11), pow(2, 12), pow(2, 13), pow(2, 14), pow(2, 15)]\n",
    "\n",
    "        param = {'gamma': g,\n",
    "                    'kernel': ['rbf'],\n",
    "                    'C': C}\n",
    "        grid_search = RandomizedSearchCV(mod, param, n_jobs=1, n_iter=100)\n",
    "        grid_search.fit(X, y)\n",
    "        clf = SVR(gamma=grid_search.best_params_[\"gamma\"], kernel=grid_search.best_params_[\n",
    "                    \"kernel\"], C=grid_search.best_params_[\"C\"])\n",
    "        clf.fit(X, y)\n",
    "        yhat = forecast_model(clf, X)\n",
    "\n",
    "        inverted = list()\n",
    "        for i in range(len(yhat)):\n",
    "            value = inverse_difference(dataset, yhat[i], len(dataset) - i)\n",
    "            inverted.append(value)\n",
    "        inverted = np.array(inverted)\n",
    "\n",
    "        forecast = inverted[-1]\n",
    "        if forecast <= 0:\n",
    "            forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "        predictions.append(forecast)\n",
    "        dataset = np.append(dataset, forecast)\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions\n",
    "\n",
    "def model_SVR_Poly(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    for i in range(tsize):\n",
    "        diff_values = difference(dataset, 1)\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        data = supervised.values\n",
    "\n",
    "        if train_flag == 1:\n",
    "            train = data[0:-tsize]\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        X, y = train[:, 0:-1].reshape(-1, 1), train[:, -1]\n",
    "\n",
    "        mod = SVR()\n",
    "        g = list(np.linspace(0.0001, 1, 1000))\n",
    "        C = list(np.linspace(0.01, 10, 25))\n",
    "        param = {\"kernel\": [\"poly\"],\n",
    "                    \"degree\": range(10, 30, 1),\n",
    "                    \"gamma\": g,\n",
    "                    \"C\": C}\n",
    "        random_search = RandomizedSearchCV(\n",
    "            mod, param, n_jobs=1, n_iter=100)\n",
    "        random_search.fit(X, y)\n",
    "        clf = SVR(kernel=random_search.best_params_[\"kernel\"], degree=random_search.best_params_[\n",
    "                    \"degree\"], gamma=random_search.best_params_[\"gamma\"], C=random_search.best_params_[\"C\"])\n",
    "\n",
    "        clf.fit(X, y)\n",
    "        yhat = forecast_model(clf, X)\n",
    "\n",
    "        inverted = list()\n",
    "        for i in range(len(yhat)):\n",
    "            value = inverse_difference(dataset, yhat[i], len(dataset) - i)\n",
    "            inverted.append(value)\n",
    "        inverted = np.array(inverted)\n",
    "\n",
    "        forecast = inverted[-1]\n",
    "        if forecast <= 0:\n",
    "            forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "        predictions.append(forecast)\n",
    "        dataset = np.append(dataset, forecast)\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions\n",
    "\n",
    "def model_DecisionTree(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    for i in range(tsize):\n",
    "        diff_values = difference(dataset, 1)\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        data = supervised.values\n",
    "\n",
    "        if train_flag == 1:\n",
    "            train = data[0:-tsize]\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        X, y = train[:, 0:-1].reshape(-1, 1), train[:, -1]\n",
    "\n",
    "        dtr = DecisionTreeRegressor()\n",
    "        param_tree = {\"max_depth\": [3, None],\n",
    "                        \"min_samples_leaf\": sp_randint(1, 11),\n",
    "                        \"criterion\": [\"mse\"],\n",
    "                        \"splitter\": [\"best\", \"random\"],\n",
    "                        \"max_features\": [\"auto\", \"sqrt\", None]}\n",
    "\n",
    "        gridDT = RandomizedSearchCV(dtr, param_tree, n_jobs=1, n_iter=100)\n",
    "        gridDT.fit(X, y)\n",
    "        clf = DecisionTreeRegressor(criterion=gridDT.best_params_[\"criterion\"], splitter=gridDT.best_params_[\"splitter\"], max_features=gridDT.best_params_[\n",
    "                                    \"max_features\"], max_depth=gridDT.best_params_[\"max_depth\"], min_samples_leaf=gridDT.best_params_[\"min_samples_leaf\"])\n",
    "\n",
    "        clf.fit(X, y)\n",
    "        yhat = forecast_model(clf, X)\n",
    "\n",
    "        inverted = list()\n",
    "        for i in range(len(yhat)):\n",
    "            value = inverse_difference(dataset, yhat[i], len(dataset) - i)\n",
    "            inverted.append(value)\n",
    "        inverted = np.array(inverted)\n",
    "\n",
    "        forecast = inverted[-1]\n",
    "        if forecast <= 0:\n",
    "            forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "        predictions.append(forecast)\n",
    "        dataset = np.append(dataset, forecast)\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions\n",
    "\n",
    "def model_RandomForest(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    for i in range(tsize):\n",
    "        diff_values = difference(dataset, 1)\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        data = supervised.values\n",
    "\n",
    "        if train_flag == 1:\n",
    "            train = data[0:-tsize]\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        X, y = train[:, 0:-1].reshape(-1, 1), train[:, -1]\n",
    "        rfr = RandomForestRegressor()\n",
    "        param_forest = {\"n_estimators\": range(10, 1000, 100),\n",
    "                        \"criterion\": [\"mse\"],\n",
    "                        \"bootstrap\": [True, False],\n",
    "                        \"warm_start\": [True, False]\n",
    "                        }\n",
    "        gridRF = RandomizedSearchCV(\n",
    "            rfr, param_forest, n_jobs=1, n_iter=100)\n",
    "        gridRF.fit(X, y)\n",
    "        yhat = forecast_model(gridRF, X)\n",
    "\n",
    "        inverted = list()\n",
    "        for i in range(len(yhat)):\n",
    "            value = inverse_difference(dataset, yhat[i], len(dataset) - i)\n",
    "            inverted.append(value)\n",
    "        inverted = np.array(inverted)\n",
    "\n",
    "        forecast = inverted[-1]\n",
    "        if forecast <= 0:\n",
    "            forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "        predictions.append(forecast)\n",
    "        dataset = np.append(dataset, forecast)\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions\n",
    "\n",
    "def model_Ridge(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    for i in range(tsize):\n",
    "        diff_values = difference(dataset, 1)\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        data = supervised.values\n",
    "\n",
    "        if train_flag == 1:\n",
    "            train = data[0:-tsize]\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        X, y = train[:, 0:-1].reshape(-1, 1), train[:, -1]\n",
    "        rdg = Ridge()\n",
    "        para_ridge = {\"alpha\": list(np.linspace(0.000000001, 10000, 1000000)),\n",
    "                        \"fit_intercept\": [True, False],\n",
    "                        \"normalize\": [True, False],\n",
    "                        \"solver\": [\"auto\"]}\n",
    "        random_rdg = RandomizedSearchCV(\n",
    "            rdg, para_ridge, n_jobs=1, n_iter=100)\n",
    "        random_rdg.fit(X, y)\n",
    "        clf = Ridge(alpha=random_rdg.best_params_[\"alpha\"], fit_intercept=random_rdg.best_params_[\n",
    "                    \"fit_intercept\"], normalize=random_rdg.best_params_[\"normalize\"], solver=random_rdg.best_params_[\"solver\"])\n",
    "\n",
    "        clf.fit(X, y)\n",
    "        yhat = forecast_model(clf, X)\n",
    "\n",
    "        inverted = list()\n",
    "        for i in range(len(yhat)):\n",
    "            value = inverse_difference(dataset, yhat[i], len(dataset) - i)\n",
    "            inverted.append(value)\n",
    "        inverted = np.array(inverted)\n",
    "\n",
    "        forecast = inverted[-1]\n",
    "        if forecast <= 0:\n",
    "            forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "        predictions.append(forecast)\n",
    "        dataset = np.append(dataset, forecast)\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions\n",
    "\n",
    "def model_Lasso(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    for i in range(tsize):\n",
    "        diff_values = difference(dataset, 1)\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        data = supervised.values\n",
    "\n",
    "        if train_flag == 1:\n",
    "            train = data[0:-tsize]\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        X, y = train[:, 0:-1].reshape(-1, 1), train[:, -1]\n",
    "        lass = Lasso()\n",
    "        param_lass = {\"alpha\": list(np.linspace(0.000000001, 100, 1000)),\n",
    "                        \"fit_intercept\": [True, False],\n",
    "                        \"normalize\": [True, False],\n",
    "                        \"selection\": [\"random\"]}\n",
    "        random_lass = RandomizedSearchCV(\n",
    "            lass, param_lass, n_jobs=1, n_iter=100)\n",
    "        random_lass.fit(X, y)\n",
    "        clf = Lasso(alpha=random_lass.best_params_[\"alpha\"], fit_intercept=random_lass.best_params_[\n",
    "                    \"fit_intercept\"], normalize=random_lass.best_params_[\"normalize\"], selection=random_lass.best_params_[\"selection\"])\n",
    "\n",
    "        clf.fit(X, y)\n",
    "        yhat = forecast_model(clf, X)\n",
    "\n",
    "        inverted = list()\n",
    "        for i in range(len(yhat)):\n",
    "            value = inverse_difference(dataset, yhat[i], len(dataset) - i)\n",
    "            inverted.append(value)\n",
    "        inverted = np.array(inverted)\n",
    "\n",
    "        forecast = inverted[-1]\n",
    "        if forecast <= 0:\n",
    "            forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "        predictions.append(forecast)\n",
    "        dataset = np.append(dataset, forecast)\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions\n",
    "\n",
    "def model_ElasticNet(dataset=[], tsize=0, order=(0, 0, 0), train_flag=0):\n",
    "    predictions = []\n",
    "    for i in range(tsize):\n",
    "        diff_values = difference(dataset, 1)\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        data = supervised.values\n",
    "\n",
    "        if train_flag == 1:\n",
    "            train = data[0:-tsize]\n",
    "        else:\n",
    "            train = data\n",
    "\n",
    "        X, y = train[:, 0:-1].reshape(-1, 1), train[:, -1]\n",
    "        elas = ElasticNet()\n",
    "        param = {\"alpha\": list(np.linspace(0.000000001, 100, 100000)),\n",
    "                    \"l1_ratio\": list(np.linspace(0.000001, 100, 1000)),\n",
    "                    \"fit_intercept\": [True, False],\n",
    "                    \"normalize\": [True, False],\n",
    "                    \"precompute\": [True, False]}\n",
    "        random_elas = RandomizedSearchCV(elas, param, n_jobs=1, n_iter=100)\n",
    "        random_elas.fit(X, y)\n",
    "        clf = ElasticNet(alpha=random_elas.best_params_[\"alpha\"], l1_ratio=random_elas.best_params_[\"l1_ratio\"], fit_intercept=random_elas.best_params_[\"fit_intercept\"],\n",
    "                            normalize=random_elas.best_params_[\"normalize\"], precompute=random_elas.best_params_[\"precompute\"])\n",
    "\n",
    "        clf.fit(X, y)\n",
    "        yhat = forecast_model(clf, X)\n",
    "\n",
    "        inverted = list()\n",
    "        for i in range(len(yhat)):\n",
    "            value = inverse_difference(dataset, yhat[i], len(dataset) - i)\n",
    "            inverted.append(value)\n",
    "        inverted = np.array(inverted)\n",
    "\n",
    "        forecast = inverted[-1]\n",
    "        if forecast <= 0:\n",
    "            forecast = weighted_moving_average(dataset, 1, 3)[0]\n",
    "        predictions.append(forecast)\n",
    "        dataset = np.append(dataset, forecast)\n",
    "    predictions = [int(i) for i in predictions]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "VtrJ9tP0Zkoy"
   },
   "outputs": [],
   "source": [
    "# In[37]:\n",
    "\n",
    "def model_predict(best_algo, best_order, data, forecast_period, rept=0):\n",
    "\n",
    "    predictions = []\n",
    "    ML_models = models_ML()\n",
    "    ARIMA_models = init_ARIMA_models(best_order)\n",
    "\n",
    "    if best_algo in ML_models.keys():\n",
    "        if best_algo == 'GR_LR':\n",
    "            predictions = model_LinearRegression(\n",
    "                data.values, forecast_period, best_order)\n",
    "        elif best_algo == 'SVR_Sigmoid':\n",
    "            predictions = model_SVR_Sigmoid(\n",
    "                data.values, forecast_period, best_order)\n",
    "        elif best_algo == 'SVR_RBF':\n",
    "            predictions = model_SVR_RBF(\n",
    "                data.values, forecast_period, best_order)\n",
    "        else:\n",
    "            test_shape_add = test_shape_adder(best_algo)\n",
    "            test_shape_fin = forecast_period+test_shape_add\n",
    "            predictions, rmse = model_ML(dataset=data.values, tsize=forecast_period,\n",
    "                                            test_shape=test_shape_fin, model=ML_models[best_algo], order=best_order)\n",
    "            if test_shape_add > 0:\n",
    "                st = test_shape_fin-1\n",
    "                end = test_shape_add-1\n",
    "                predictions = predictions[-st:-end]\n",
    "\n",
    "    elif best_algo in ARIMA_models.keys():\n",
    "        #        print(\"lolllllllzzz\",ARIMA_models[best_algo])\n",
    "        predictions, rmse = model_ARIMA(\n",
    "            best_algo, train=data.values, test_shape=forecast_period, order=ARIMA_models[best_algo], train_flag=0)\n",
    "\n",
    "    elif best_algo in ['SES', 'HWES']:\n",
    "        predictions, rmse = model_ES(\n",
    "            best_algo, train=data.values, test_shape=forecast_period, train_flag=0)\n",
    "    elif best_algo in ['naive', 'naive2', 'naive3', 'naive6', 'naive12', 'naive12wa', 'naive6wa']:\n",
    "        predictions, rmse = model_Naive(\n",
    "            best_algo, data.values, forecast_period, best_order, rept, train_flag=0)\n",
    "\n",
    "    elif best_algo in ['sma', 'wma']:\n",
    "        predictions, rmse = model_MA(\n",
    "            best_algo, data.values, forecast_period, train_flag=0)\n",
    "    else:\n",
    "        predictions = [0]*forecast_period\n",
    "    predictions = [j if j>0 else data.tail(i+12)[data[0].tail(i+12)>0].values[0][0] for i,j in enumerate(predictions)]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "tS7IWfPMZn8-"
   },
   "outputs": [],
   "source": [
    "# In[38]:\n",
    "\n",
    "def calculate_validation_facc(expected, predictions):\n",
    "    validation_facc = []\n",
    "    for i in range(len(expected)):\n",
    "        a = int(expected[i])\n",
    "        b = int(predictions[i])\n",
    "        value = ((1 - (np.abs(a - b)) / (a+(a == 0))) * 100)\n",
    "\n",
    "        if np.isnan(value) == True or np.isfinite(value) == False:\n",
    "            value = 0\n",
    "        validation_facc.append(float(format(value, '.3f')))\n",
    "\n",
    "    validation_facc = [0 if i < 0 else i for i in validation_facc]\n",
    "    print(\"Validation Accuracy\")\n",
    "    return validation_facc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "0Yka2sYiZp5J"
   },
   "outputs": [],
   "source": [
    "# In[39]:\n",
    "\n",
    "def test_shape_adder(key):\n",
    "    if key == 'knn':\n",
    "        test_shape_add = 0\n",
    "    elif key == 'lasso':\n",
    "        test_shape_add = 0\n",
    "    elif key == 'lr':\n",
    "        test_shape_add = 0\n",
    "    elif key == 'ridge' or key == 'en' or key == 'llars' or key == 'pa':  # or key == 'huber'\n",
    "        test_shape_add = 0\n",
    "    elif key == 'cart' or key == 'extra' or key == 'svmr' or key == 'ada':\n",
    "        test_shape_add = 2\n",
    "    elif key == 'bag' or key == 'rf' or key == 'et' or key == 'gbm':\n",
    "        test_shape_add = 3\n",
    "    else:\n",
    "        test_shape_add = 0\n",
    "\n",
    "    return test_shape_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "H6Wfel1lZsTN"
   },
   "outputs": [],
   "source": [
    "# In[40]:\n",
    "\n",
    "def calculate_validation_mape(expected, predictions):\n",
    "    validation_mape = []\n",
    "    for i in range(len(expected)):\n",
    "        a = int(expected[i])\n",
    "        b = int(predictions[i])\n",
    "        if a != 0:\n",
    "            value = np.abs((a-b)/a) * 100\n",
    "            # value = (((np.abs(a - b)) / (a)) * 100)\n",
    "\n",
    "        elif b != 0:\n",
    "            # value = ((np.abs(a - b)) / (b)) * 100\n",
    "            value = np.abs((a-b)/b) * 100\n",
    "\n",
    "\n",
    "        elif a == 0 and b == 0:\n",
    "            value = 0\n",
    "\n",
    "        if np.isnan(value) == True or np.isfinite(value) == False:\n",
    "            value = 0\n",
    "\n",
    "        validation_mape.append(float(format(value, '.3f')))\n",
    "\n",
    "    validation_mape = [0 if i < 0 else i for i in validation_mape]\n",
    "\n",
    "    return validation_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "2EkLcXYkZug9"
   },
   "outputs": [],
   "source": [
    "# In[41]:\n",
    "\n",
    "def calculate_facc(y_true, y_pred):\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    facc = 1 - (sum(np.abs(y_true - y_pred)) / sum(y_true))\n",
    "    if np.isnan(facc) == True or np.isfinite(facc) == False:\n",
    "        facc = 0\n",
    "    return facc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "XZI_o-UiZxAi"
   },
   "outputs": [],
   "source": [
    "# In[42]:\n",
    "\n",
    "def calculate_weight(error1, error2):\n",
    "    if error1 == 0.0:\n",
    "        a = 1\n",
    "        b = 0\n",
    "    elif error2 == 0.0:\n",
    "        b = 1\n",
    "        a = 0\n",
    "    else:\n",
    "        a = 1/error1\n",
    "        b = 1/error2\n",
    "    weight = a/(a+b)\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "yf4HPZMiZza2"
   },
   "outputs": [],
   "source": [
    "# In[43]:\n",
    "\n",
    "\n",
    "def weight_calculation(data, best_models,rmse_ML,rmse_TS):\n",
    "    itr = 5\n",
    "    weight_ts = 0\n",
    "    weight_ml = 0\n",
    "    for i,(ts,ml) in enumerate(zip(rmse_TS[best_models[1]],rmse_ML[best_models[0]])):\n",
    "        rmse_ts = ts\n",
    "        rmse_ml = ml\n",
    "        weight_ts += calculate_weight(rmse_ts, rmse_ml)\n",
    "        weight_ml += calculate_weight(rmse_ml, rmse_ts)\n",
    "    weight_ts = weight_ts/3\n",
    "    weight_ml = weight_ml/3\n",
    "    return weight_ts, weight_ml\n",
    "\n",
    "# %%\n",
    "def statistical_baseline_model(data,period, f_type):\n",
    "    \"\"\"\n",
    "    This Function takes the two input:\n",
    "    1. Data - The format of the data is dataframe in which the rows are the values\n",
    "    2. Forecast period as an integer\n",
    "    3. Period is the Forecast Type, Monthly, weekly or daily\n",
    "\n",
    "    Output:\n",
    "    This outputs the forecast for the period given in a DataFrame manner\n",
    "    \"\"\"\n",
    "    f_type = f_type.lower()\n",
    "\n",
    "    if f_type == 'M' or f_type == 'MS':\n",
    "        if period >= 12:\n",
    "            period = 12\n",
    "        else:\n",
    "            period = period\n",
    "    elif f_type == 'W':\n",
    "        if period >= 52:\n",
    "            period = 52\n",
    "        else:\n",
    "            period = period\n",
    "    else:\n",
    "        if period >= 365:\n",
    "            period = 365\n",
    "        else:\n",
    "            period = period\n",
    "\n",
    "    a,b = data.shape\n",
    "    zeros = []\n",
    "    if a < 3*period:\n",
    "        b = 3*period - a\n",
    "        for num in range(b):\n",
    "            zeros.append(0)\n",
    "        for num in data.values:\n",
    "            try:\n",
    "                zeros.append(np.asscalar(num))\n",
    "            except:\n",
    "                zeros.append(num)\n",
    "        dataset = pd.DataFrame(zeros)\n",
    "    else:\n",
    "        dataset = pd.DataFrame(data.values)\n",
    "\n",
    "    forecast1 = dataset.tail(period).values\n",
    "    year_1 = dataset.tail(period).head(period).values\n",
    "    year_2 = dataset.tail(2*period).head(period).values\n",
    "    year_3 = dataset.tail(3*period).head(period).values\n",
    "\n",
    "    mean1 = year_1.mean()\n",
    "    mean2 = year_2.mean()\n",
    "    mean3 = year_3.mean()\n",
    "\n",
    "    fmean = forecast1.mean()\n",
    "    f_avg = (mean1 + mean2 + mean3)/3.0\n",
    "\n",
    "    difference = f_avg - fmean\n",
    "    new_forecast = forecast1 - difference\n",
    "    final_forecast = []\n",
    "    for num in new_forecast:\n",
    "        if num < 0:\n",
    "            final_forecast.append([0.0])\n",
    "        else:\n",
    "            final_forecast.append(num)\n",
    "    return(pd.DataFrame(final_forecast).head(period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDkr1ay4Z4JG",
    "outputId": "8b531f5f-21fd-4dc2-b39b-cebdba35da48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Running SKU 1: Close...\n",
      "Nan less than 60%\n",
      "BEST ORDER : (20, 1, 31)\n",
      "lr\n",
      "RMSE FOR lr: 1 \n",
      "RMSE FOR lr: 2 \n",
      "RMSE FOR lr: 2 \n",
      "lasso\n",
      "RMSE FOR lasso: 1 \n",
      "RMSE FOR lasso: 3 \n",
      "RMSE FOR lasso: 3 \n",
      "ridge\n",
      "RMSE FOR ridge: 1 \n",
      "RMSE FOR ridge: 2 \n",
      "RMSE FOR ridge: 2 \n",
      "en\n",
      "RMSE FOR en: 0 \n",
      "RMSE FOR en: 2 \n",
      "RMSE FOR en: 3 \n",
      "llars\n",
      "RMSE FOR llars: 41 \n",
      "RMSE FOR llars: 43 \n",
      "RMSE FOR llars: 43 \n",
      "pa\n",
      "RMSE FOR pa: 6 \n",
      "RMSE FOR pa: 4 \n",
      "RMSE FOR pa: 9 \n",
      "knn\n",
      "RMSE FOR knn: 1 \n",
      "RMSE FOR knn: 3 \n",
      "RMSE FOR knn: 3 \n",
      "cart\n",
      "RMSE FOR cart: 1 \n",
      "RMSE FOR cart: 3 \n",
      "RMSE FOR cart: 2 \n",
      "extra\n",
      "RMSE FOR extra: 1 \n",
      "RMSE FOR extra: 3 \n",
      "RMSE FOR extra: 2 \n",
      "svmr\n",
      "RMSE FOR svmr: 52 \n",
      "RMSE FOR svmr: 53 \n",
      "RMSE FOR svmr: 53 \n",
      "ada\n",
      "RMSE FOR ada: 2 \n",
      "RMSE FOR ada: 2 \n",
      "RMSE FOR ada: 2 \n",
      "bag\n",
      "RMSE FOR bag: 0 \n",
      "RMSE FOR bag: 2 \n",
      "RMSE FOR bag: 3 \n",
      "rf\n",
      "RMSE FOR rf: 1 \n",
      "RMSE FOR rf: 2 \n",
      "RMSE FOR rf: 3 \n",
      "gbm\n",
      "RMSE FOR gbm: 0 \n",
      "RMSE FOR gbm: 2 \n",
      "RMSE FOR gbm: 3 \n",
      "KEY! AR\n",
      "RMSE FOR AR: 3 \n",
      "RMSE FOR AR: 5 \n",
      "RMSE FOR AR: 6 \n",
      "KEY! MA\n",
      "RMSE FOR MA: 3 \n",
      "RMSE FOR MA: 5 \n",
      "RMSE FOR MA: 6 \n",
      "KEY! ARMA\n",
      "RMSE FOR ARMA: 3 \n",
      "RMSE FOR ARMA: 5 \n",
      "RMSE FOR ARMA: 6 \n",
      "KEY! ARIMA\n",
      "RMSE FOR ARIMA: 3 \n",
      "RMSE FOR ARIMA: 5 \n",
      "RMSE FOR ARIMA: 6 \n",
      "KEY! spl_ARIMA\n",
      "RMSE FOR spl_ARIMA: 12 \n",
      "RMSE FOR spl_ARIMA: 9 \n",
      "RMSE FOR spl_ARIMA: 7 \n",
      "KEY! STL_ARIMA\n",
      "RMSE FOR STL_ARIMA: 4 \n",
      "RMSE FOR STL_ARIMA: 5 \n",
      "RMSE FOR STL_ARIMA: 8 \n",
      "RMSE FOR SES: 5 \n",
      "RMSE FOR SES: 4 \n",
      "RMSE FOR SES: 3 \n",
      "RMSE FOR HWES: 5 \n",
      "RMSE FOR HWES: 4 \n",
      "RMSE FOR HWES: 3 \n",
      "RMSE FOR naive: 18 \n",
      "RMSE FOR naive: 22 \n",
      "RMSE FOR naive: 27 \n",
      "RMSE FOR naive_rept: 2 \n",
      "RMSE FOR naive_rept: 6 \n",
      "RMSE FOR naive_rept: 5 \n",
      "RMSE FOR naive3: 3 \n",
      "RMSE FOR naive3: 6 \n",
      "RMSE FOR naive3: 7 \n",
      "RMSE FOR naive6: 4 \n",
      "RMSE FOR naive6: 6 \n",
      "RMSE FOR naive6: 9 \n",
      "RMSE FOR naive12: 5 \n",
      "RMSE FOR naive12: 1 \n",
      "RMSE FOR naive12: 4 \n",
      "RMSE FOR naive12wa: 11 \n",
      "RMSE FOR naive12wa: 10 \n",
      "RMSE FOR naive12wa: 11 \n",
      "RMSE FOR sma: 3 \n",
      "RMSE FOR sma: 5 \n",
      "RMSE FOR sma: 6 \n",
      "RMSE FOR wma: 3 \n",
      "RMSE FOR wma: 5 \n",
      "RMSE FOR wma: 6 \n",
      "Moving_Average done\n",
      "Modeling done\n",
      "RMSE FOR Croston: 6 \n"
     ]
    }
   ],
   "source": [
    "error_sku = []\n",
    "\n",
    "def training(datasets, forecast_period):\n",
    "    forecast_results = []\n",
    "    num = 0\n",
    "    col = ['sku', 'model', 'rmse', 'mape']  # changed 2\n",
    "    fc = []\n",
    "    for i in range(1, forecast_period+1):\n",
    "        fc.append('forecast'+str(i))\n",
    "    col.extend(fc)\n",
    "    output_all = pd.DataFrame(columns=col)\n",
    "    models_out = pd.DataFrame(columns=['sku', 'model_ts', 'model_ml'])\n",
    "    output_best = pd.DataFrame(columns=col)\n",
    "\n",
    "    for incr, sku in enumerate(datasets):\n",
    "        try:\n",
    "            num += 1\n",
    "\n",
    "            print(\"------------------------------------------------------------\")\n",
    "            print(\"Running SKU %d: %s...\" % (num, sku))\n",
    "            stp_copy = copy.deepcopy(datasets[sku].T)\n",
    "\n",
    "            raw_data = copy.deepcopy(datasets[sku].T)\n",
    "            output = init_output(forecast_period, raw_data)\n",
    "\n",
    "            dataset = raw_data.copy()\n",
    "\n",
    "            dataset = dataset[:-1]\n",
    "            interval = find_interval(dataset.index)\n",
    "\n",
    "            logging.info(interval.days)\n",
    "\n",
    "            if((dataset['sales'] == 0).all() == True or (set([math.isnan(x) for x in dataset['sales']]) == {True})):\n",
    "                print(\"All zeros/NaNs\")\n",
    "                forecast = [0] * forecast_period\n",
    "                output['forecast_values'] = assign_dates(\n",
    "                    forecast, 'forecast', dataset.tail(1), sku=sku)  # changed\n",
    "                output['facc'], output['mape'], output['bias'] = calculate_forecast_accuracy(\n",
    "                    raw_data.iloc[-1], forecast[0])\n",
    "\n",
    "                forecast_results = output_forecast(\n",
    "                    sku, dataset, datasets[sku].T, output, forecast_results)\n",
    "                continue\n",
    "\n",
    "            sku_data = dataset.astype(np.float32)\n",
    "            sku_data = read_from_first_sales(sku_data['sales'])\n",
    "\n",
    "            size, sparse_size, freq = get_bucket_size(interval)\n",
    "\n",
    "            test_nan = pd.DataFrame(sku_data[-freq:])\n",
    "            test_nan = test_nan['sales']\n",
    "\n",
    "    # if last 1 year is NaN, impute data with zero and forecast is MA(6)\n",
    "\n",
    "            if sum(test_nan.isnull()) >= freq:\n",
    "                print(\"Last 1 year NaN\")\n",
    "                sku_data = data_imputation_zero(test_nan)\n",
    "                forecast = moving_average(sku_data, forecast_period, 6)\n",
    "                output['forecast_values'] = assign_dates(\n",
    "                    forecast, 'forecast', dataset.tail(1), sku=sku)\n",
    "                output['facc'], output['mape'], output['bias'] = calculate_forecast_accuracy(\n",
    "                    raw_data.iloc[-1], forecast[0])\n",
    "\n",
    "                forecast_results = output_forecast(\n",
    "                    sku, dataset, sku_data, output, forecast_results)\n",
    "                continue\n",
    "\n",
    "    # if # NaNs more than 60% impute with 0 else impute with values\n",
    "\n",
    "            if sum(pd.isnull(sku_data)) > (0.6*len(sku_data)):\n",
    "                print(\"Nan Greater than 60%\")\n",
    "                sku_data = data_imputation_zero(sku_data)\n",
    "\n",
    "            else:\n",
    "                print(\"Nan less than 60%\")\n",
    "                sku_data = data_imputation(sku_data, freq)\n",
    "                sku_data = sku_data[0]\n",
    "\n",
    "            sku_data = read_from_first_sales(sku_data)\n",
    "\n",
    "    # After reading from first non-zero if data is insufficient ---> weighted MA(3)\n",
    "\n",
    "            if len(sku_data) < 20:\n",
    "                try:\n",
    "                    print(\"Weighted Moving Average\")\n",
    "                    forecast = weighted_moving_average(\n",
    "                        sku_data, forecast_period, 3)\n",
    "                    output['forecast_values'] = assign_dates(\n",
    "                        forecast, 'forecast', dataset.tail(1), sku=sku)\n",
    "                    output['facc'], output['mape'], output['bias'] = calculate_forecast_accuracy(\n",
    "                        raw_data.iloc[-1], forecast[0])\n",
    "\n",
    "                    forecast_results = output_forecast(\n",
    "                        sku, dataset, sku_data, output, forecast_results)\n",
    "                except:\n",
    "                    print(\"Less than 3\")\n",
    "                    forecast = moving_average(\n",
    "                        sku_data, forecast_period, len(sku_data))\n",
    "                    output['forecast_values'] = assign_dates(\n",
    "                        forecast, 'forecast', dataset.tail(1))\n",
    "                    output['facc'], output['mape'], output['bias'] = calculate_forecast_accuracy(\n",
    "                        raw_data.iloc[-1], forecast[0])\n",
    "\n",
    "                    forecast_results = output_forecast(\n",
    "                        sku, dataset, sku_data, output, forecast_results)\n",
    "\n",
    "                continue\n",
    "\n",
    "            data_copy = sku_data.copy()\n",
    "            data_copy = np.array(data_copy)\n",
    "\n",
    "            index1, index2, sflag1, sflag2 = Sesonal_detection(sku_data)\n",
    "            sku_data = outlier_treatment_tech(sku_data, interval, size)\n",
    "            sku_data = np.array(sku_data[0])\n",
    "\n",
    "            # print(sku_data)\n",
    "\n",
    "            if sflag1 == 1:\n",
    "                sku_data[index1] = data_copy[index1]\n",
    "            if sflag2 == 1:\n",
    "                sku_data[index2] = data_copy[index2]\n",
    "            else:\n",
    "                sku_data = sku_data\n",
    "\n",
    "            sku_data = pd.DataFrame(sku_data)\n",
    "\n",
    "            # Testing Stationarity\n",
    "            d = 0\n",
    "            df_test_result = dickeyfullertest(\n",
    "                sku_data.T.squeeze())  # pd.Series(sku_data[0])\n",
    "\n",
    "            while df_test_result == 0:\n",
    "                d += 1\n",
    "                if d == 1:\n",
    "                    new_data = difference(sku_data[0].tolist())\n",
    "                else:\n",
    "                    new_data = difference(new_data)\n",
    "                df_test_result = dickeyfullertest(new_data)\n",
    "            sample = np.array(sku_data)\n",
    "            repeat = check_repetition(sample, freq, 1, len(sample))\n",
    "            # Finding p and q value\n",
    "            try:\n",
    "                if d == 0:\n",
    "                    p1, ps, pl = acf_plot(sku_data, freq)\n",
    "                    q = pacf_plot(sku_data, freq)\n",
    "                    data = sku_data\n",
    "                else:\n",
    "\n",
    "                    p, ps, pl = acf_plot(new_data, freq)\n",
    "                    q = pacf_plot(new_data, freq)\n",
    "                    data = new_data\n",
    "\n",
    "                if repeat in ps:\n",
    "                    p = repeat\n",
    "                elif repeat in pl:\n",
    "                    p = repeat\n",
    "                else:\n",
    "                    p = pl[0]\n",
    "                if p > freq:\n",
    "                    p = freq\n",
    "            except:\n",
    "                p = 1\n",
    "                q = 1\n",
    "                data = sku_data\n",
    "\n",
    "            data = sku_data\n",
    "            best_order = (p, d, q)\n",
    "            print(\"BEST ORDER :\", best_order)\n",
    "            tsize = 5\n",
    "            expected = data[-tsize:].reset_index(drop=True)\n",
    "            expected = [float(i) for i in expected.values]\n",
    "            train_6wa = sku_data[0:-tsize]\n",
    "            predictions_ML, rmse_ML = time_series_using_ml(\n",
    "                sku_data, tsize, best_order)\n",
    "            rmse_ARIMA, rmse_ES, rmse_naive, rmse_ma, predictions_ARIMA, predictions_ES, predictions_naive, predictions_ma = time_series_models(\n",
    "                freq, sku_data, data, tsize, best_order)\n",
    "            print(\"Modeling done\")\n",
    "            rmse_TS = rmse_ARIMA.copy()\n",
    "            rmse_TS.update(rmse_ES)\n",
    "            rmse_TS.update(rmse_naive)\n",
    "            rmse_TS.update(rmse_ma)\n",
    "\n",
    "            predictions = predictions_ML\n",
    "            predictions.update(predictions_ARIMA)\n",
    "            predictions.update(predictions_ES)\n",
    "            predictions.update(predictions_naive)\n",
    "            predictions.update(predictions_ma)\n",
    "\n",
    "            rmse_Croston, predictions_Croston = Croston_TSB(\n",
    "                sku_data, tsize)\n",
    "            rmse_TS.update(rmse_Croston)\n",
    "            predictions.update(predictions_Croston)\n",
    "\n",
    "            for key in rmse_ML:\n",
    "\n",
    "                forecast_period_loc = forecast_period\n",
    "                models_to_shift = [\"lr\", \"ridge\",\n",
    "                                    \"lasso\", \"en\", \"llars\", \"pa\", \"knn\"]\n",
    "                model_to_be_shifted = True if key in models_to_shift else False\n",
    "                if model_to_be_shifted:\n",
    "                    forecast_period_loc = forecast_period_loc + 1\n",
    "                model_forecast = model_predict(\n",
    "                    key, best_order, data, forecast_period_loc)\n",
    "                if model_to_be_shifted:\n",
    "                    model_forecast = model_forecast[1:]\n",
    "\n",
    "            \n",
    "                model_forecast = [0 if i < 0 else int(\n",
    "                    i) for i in model_forecast]\n",
    "                rmse_val = np.mean(rmse_ML[key])\n",
    "                mape_val = np.mean(calculate_validation_mape(\n",
    "                    expected, predictions[key]))\n",
    "                output_dict = {'sku': sku, 'model': key,\n",
    "                                'rmse': rmse_val, 'mape': mape_val}\n",
    "                for i in range(1, forecast_period+1):\n",
    "                    output_dict['forecast' +\n",
    "                                str(i)] = model_forecast[i-1]  # changed3\n",
    "\n",
    "                output_all = output_all.append(\n",
    "                    output_dict, ignore_index=True)\n",
    "\n",
    "            for key in rmse_TS:\n",
    "                model_forecast = model_predict(\n",
    "                    key, best_order, data, forecast_period)\n",
    "                model_forecast = [0 if i < 0 else int(\n",
    "                    i) for i in model_forecast]\n",
    "                rmse_val = np.mean(rmse_TS[key])\n",
    "                mape_val = np.mean(calculate_validation_mape(\n",
    "                    expected, predictions[key]))\n",
    "                output_dict = {'sku': sku, 'model': key,\n",
    "                                'rmse': rmse_val, 'mape': mape_val}\n",
    "                for i in range(1, forecast_period+1):\n",
    "                    output_dict['forecast' +\n",
    "                                str(i)] = model_forecast[i-1]  # changed4\n",
    "                output_all = output_all.append(\n",
    "                    output_dict, ignore_index=True)\n",
    "\n",
    "            rmse_vol_ml = dict()\n",
    "            for key in rmse_ML:\n",
    "                mean = np.mean(rmse_ML[key])\n",
    "                rmse_vol_ml[key] = mean\n",
    "\n",
    "            rmse_vol_ts = dict()\n",
    "            for key in rmse_TS:\n",
    "                mean = np.mean(rmse_TS[key])\n",
    "                rmse_vol_ts[key] = mean\n",
    "\n",
    "\n",
    "            # Top 3 models\n",
    "            best_models_ml = sorted(\n",
    "                rmse_vol_ml, key=rmse_vol_ml.get, reverse=False)[:3]\n",
    "            best_models_ts = sorted(\n",
    "                rmse_vol_ts, key=rmse_vol_ts.get, reverse=False)[:3]\n",
    "\n",
    "            bias_ml = []\n",
    "            accuracy_ml = []\n",
    "            for model in best_models_ml:\n",
    "                bias_ml.append(\n",
    "                    (sum(expected) - sum(predictions[model]))/len(expected))\n",
    "                accuracy_ml.append(calculate_facc(\n",
    "                    expected, predictions[model]))\n",
    "            bias_ml = [float(format(i, '.3f')) for i in bias_ml]\n",
    "            accuracy_ml = [float(format(i, '.3f')) for i in accuracy_ml]\n",
    "\n",
    "            bias_ts = []\n",
    "            accuracy_ts = []\n",
    "            for model in best_models_ts:\n",
    "                bias_ts.append(\n",
    "                    (sum(expected) - sum(predictions[model]))/len(expected))\n",
    "                accuracy_ts.append(calculate_facc(\n",
    "                    expected, predictions[model]))\n",
    "            bias_ts = [float(format(i, '.3f')) for i in bias_ts]\n",
    "            accuracy_ts = [float(format(i, '.3f')) for i in accuracy_ts]\n",
    "\n",
    "            # For one ensemble\n",
    "            error_ml = min(rmse_vol_ml.values())\n",
    "            error_ts = min(rmse_vol_ts.values())\n",
    "\n",
    "            best_models = [min(rmse_vol_ml, key=lambda x: rmse_vol_ml.get(x)), min(\n",
    "                rmse_vol_ts, key=lambda x: rmse_vol_ts.get(x))]\n",
    "            print(\"BEST MODELS :\", best_models)\n",
    "            print(\"ERRORS OF BEST MODELS :\", error_ml, error_ts)\n",
    "            forecast_period_loc = forecast_period\n",
    "            models_to_shift = [\"lr\", \"ridge\",\n",
    "                                \"lasso\", \"en\", \"llars\", \"pa\", \"knn\"]\n",
    "            model_to_be_shifted = True if best_models[0] in models_to_shift else False\n",
    "            if model_to_be_shifted:\n",
    "                forecast_period_loc = forecast_period_loc + 1\n",
    "            forecast_ml = model_predict(\n",
    "                best_models[0], best_order, data, forecast_period_loc)\n",
    "            if model_to_be_shifted:\n",
    "                forecast_ml = forecast_ml[1:]\n",
    "            if best_models[1] == 'Croston':\n",
    "                rmse_Croston, forecast_ts = Croston_TSB(\n",
    "                    sku_data, forecast_period)\n",
    "                forecast_ts = forecast_ts['Croston']\n",
    "            else:\n",
    "                forecast_ts = model_predict(\n",
    "                    best_models[1], best_order, sku_data, forecast_period, repeat)\n",
    "\n",
    "            forecast_ml = [0 if i < 0 else int(i) for i in forecast_ml]\n",
    "            forecast_ts = [0 if i < 0 else int(i) for i in forecast_ts]\n",
    "\n",
    "            # print(rmse_ML,rmse_TS)\n",
    "            weight_ts, weight_ml = weight_calculation(\n",
    "                data, best_models,rmse_ML,rmse_TS)\n",
    "            print(\"weight ts:\", weight_ts)\n",
    "            print(\"weight ml:\", weight_ml)\n",
    "\n",
    "            forecast_ensemble =((np.array(forecast_ml)*weight_ml +\\\n",
    "                np.array(forecast_ts)*weight_ts)/(weight_ts+weight_ml)).tolist()\n",
    "\n",
    "            Vm = predictions[best_models[0]]\n",
    "            Vt = predictions[best_models[1]]\n",
    "\n",
    "            Ve=((np.array(Vm)*weight_ml +\\\n",
    "                np.array(Vt)*weight_ts)/(weight_ts+weight_ml)).tolist()\n",
    "\n",
    "            error_en=calculate_rmse('Ensemble', expected, Ve)\n",
    "            rmse_val=np.mean(error_en)\n",
    "            mape_val=np.mean(calculate_validation_mape(expected, Ve))\n",
    "\n",
    "            V6wa, rmse_6wa = model_Naive(\n",
    "                'naive6wa', train_6wa, tsize, (0, 0, 0), 0, train_flag=1)\n",
    "            error_6wa = np.mean(rmse_6wa)\n",
    "            forecast_6wa = model_predict(\n",
    "                'naive6wa', best_order, data, forecast_period)\n",
    "\n",
    "            error_min_model=min(error_ml,error_ts,error_en)\n",
    "\n",
    "            models_out = models_out.append(\n",
    "                {'sku': sku, 'model_ts': best_models[1], 'model_ml': best_models[0]}, ignore_index=True)\n",
    "\n",
    "            min_error = min(error_min_model, error_6wa)\n",
    "\n",
    "            best_forecast_selected = \"\"\n",
    "            if min_error == error_6wa or all(elem == forecast_ts[0] for elem in forecast_ts) == True or all(elem == forecast_ml[0] for elem in forecast_ml) == True:\n",
    "                print(\"Best forecast from six naive\")\n",
    "                forecast = forecast_6wa\n",
    "                best_forecast_selected = \"naive6wa\"\n",
    "                output['validation'] = assign_dates(\n",
    "                    V6wa, 'validation', dataset.tail(5))\n",
    "                validation_facc = calculate_validation_facc(expected, V6wa)\n",
    "                output['validation_facc'] = assign_dates(\n",
    "                    validation_facc, 'val_facc', dataset.tail(5))\n",
    "                best_rmse = min_error\n",
    "                best_mape = np.mean(\n",
    "                    abs((dataset.tail(5).values-V6wa)/dataset.tail(5).values))\n",
    "            elif min_error == error_ml:\n",
    "                print(\"Best forecast from ML\")\n",
    "                forecast = forecast_ml\n",
    "                output['validation'] = assign_dates(\n",
    "                    Vm, 'validation', dataset.tail(5))\n",
    "                validation_facc = calculate_validation_facc(expected, Vm)\n",
    "                output['validation_facc'] = assign_dates(\n",
    "                    validation_facc, 'val_facc', dataset.tail(5))\n",
    "                best_rmse = min_error\n",
    "                best_mape = np.mean(calculate_validation_mape(\n",
    "                    expected, predictions[best_models[0]]))\n",
    "                best_forecast_selected = best_models[0]\n",
    "\n",
    "            elif min_error==error_en:\n",
    "                print(\"Best forecast from Ensemble\")\n",
    "                forecast=forecast_ensemble\n",
    "                output['validation'] = assign_dates(Ve, 'validation', dataset.tail(5))\n",
    "                validation_facc = calculate_validation_facc(expected,Ve)\n",
    "                output['validation_facc'] = assign_dates(validation_facc, 'val_facc', dataset.tail(5))\n",
    "                best_rmse = min_error\n",
    "                best_mape = np.mean(calculate_validation_mape(\n",
    "                    expected, Ve))\n",
    "                best_forecast_selected = best_models[0]+\"_\"+best_models[1]+\"_ensemble\"\n",
    "            elif min_error == error_ts:\n",
    "                print(\"Best forecast from TS\")\n",
    "                forecast = forecast_ts\n",
    "                output['validation'] = assign_dates(\n",
    "                    Vt, 'validation', dataset.tail(5))\n",
    "                validation_facc = calculate_validation_facc(expected, Vt)\n",
    "                output['validation_facc'] = assign_dates(\n",
    "                    validation_facc, 'val_facc', dataset.tail(5))\n",
    "                best_rmse = min_error\n",
    "                best_mape = np.mean(calculate_validation_mape(\n",
    "                    expected, predictions[best_models[1]]))\n",
    "                best_forecast_selected = best_models[1]\n",
    "\n",
    "            date_type = {\"W\": \"W\", \"Y\": \"Y\", \"M\": \"MS\",\"D\":\"D\", \"Random\": \"MS\"}\n",
    "\n",
    "            test_base_forecast = statistical_baseline_model(sku_data.head(sku_data.shape[0]-5),\n",
    "                                        5,\n",
    "                                        date_type[find_interval_type(interval)])[0].to_list()\n",
    "\n",
    "            test_base_forecast = [j if j>0 else sku_data.tail(i+12)[sku_data[0].tail(i+12)>0].values[0][0] for i,j in enumerate(test_base_forecast)]\n",
    "\n",
    "            baseline_mape = np.mean(calculate_validation_mape(\n",
    "                    stp_copy['sales'].tail(5).to_list(),test_base_forecast))\n",
    "            baseline_rmse = calculate_rmse(\"\",\n",
    "            stp_copy['sales'].tail(5).to_list(),\n",
    "            test_base_forecast)\n",
    "\n",
    "            baseline_forecast = statistical_baseline_model(stp_copy,\n",
    "                                        forecast_period-1,\n",
    "                                        date_type[find_interval_type(interval)])[0].to_list()\n",
    "\n",
    "            baseline_forecast = [j if j>0 else sku_data.tail(i+12)[sku_data[0].tail(i+12)>0].values[0][0] for i,j in enumerate(baseline_forecast)]\n",
    "            baseline_forecast = [0] + baseline_forecast\n",
    "\n",
    "            output_dict={'sku':sku,'model':'Best Ensemble','rmse':rmse_val,'mape':mape_val}\n",
    "            for i in range(1,forecast_period+1):\n",
    "                output_dict['forecast'+str(i)]=forecast_ensemble[i-1]\n",
    "            output_all=output_all.append(output_dict,ignore_index=True)\n",
    "\n",
    "            output_dict = {'sku': sku, 'model': f\"Baseline Forecast\",\n",
    "                            'rmse':baseline_rmse , 'mape': baseline_mape}\n",
    "            for i in range(1, forecast_period+1):\n",
    "                output_dict['forecast'+str(i)] = baseline_forecast[i-1]\n",
    "            output_all = output_all.append(output_dict, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "            error_sku.append([sku,e])\n",
    "    \n",
    "    date_type = {\"W\": \"W\", \"Y\": \"Y\", \"M\": \"MS\",\"D\":\"D\", \"Random\": \"MS\"}\n",
    "    output_all.drop(columns=['forecast1'], inplace=True)  # temp fix\n",
    "    output_all['mape'] = output_all['mape'] / 10  # temp fix\n",
    "    forecast_columns_len = len(output_all.columns) - 4\n",
    "    output_all.columns = output_all.columns[:4].to_list() + pd.Series(pd.date_range(\n",
    "        start=pd.to_datetime(\n",
    "            datasets[sku].columns)[-1], periods=forecast_columns_len + 1,\n",
    "        freq=date_type[find_interval_type(interval)])).\\\n",
    "        dt.strftime(\"%m/%d/%Y\").to_list()[1:]\n",
    "    output_all[\"best_model\"] = \"\"\n",
    "    output_all[\"best_forecast\"] = \"\"\n",
    "    # return output_all\n",
    "    best_mlmodel = output_all[output_all.model.isin(models_ML().keys())].sort_values(\"mape\").drop_duplicates(\"sku\",keep='first').index\n",
    "    best_tsmodel = output_all[~output_all.model.isin(models_ML().keys())].sort_values(\"mape\").drop_duplicates(\"sku\",keep='first').index\n",
    "    best_forecast = output_all.sort_values(\"mape\").drop_duplicates(\"sku\",keep='first').index\n",
    "    output_all.loc[best_mlmodel,\"best_model\"]=\"Best ML\"\n",
    "    output_all.loc[best_tsmodel,\"best_model\"]=\"Best TS\"\n",
    "    output_all.loc[best_forecast,\"best_forecast\"] = 1\n",
    "    out_json = {}\n",
    "\n",
    "    out_json[\"dates\"] = output_all.columns[4:-2].to_list()\n",
    "    for idx, row in output_all.iterrows():\n",
    "\n",
    "        if out_json.get(row[\"sku\"]):\n",
    "            out_json[row[\"sku\"]][\"models\"].append(\n",
    "                {\n",
    "                    \"name\": model_names_mapping(row.model),\n",
    "                    \"data\": list(map(math.ceil, row.to_list()[4:-2])),\n",
    "                    \"metrics\": {\n",
    "                        \"rmse\": row.rmse,\n",
    "                        \"mape\": row.mape\n",
    "                    },\n",
    "        \n",
    "                })\n",
    "\n",
    "        else:\n",
    "            out_json[row[\"sku\"]] = {\n",
    "                \"models\": [\n",
    "                    {\n",
    "                        \"name\": model_names_mapping(row.model),\n",
    "                        \"data\": list(map(math.ceil, row.to_list()[4:-2])),\n",
    "                        \"metrics\": {\n",
    "                            \"rmse\": row.rmse,\n",
    "                            \"mape\": row.mape\n",
    "                        },\n",
    "                 \n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        if row.best_model:\n",
    "            out_json[row[\"sku\"]][\"models\"].append(\n",
    "                {\n",
    "                    \"name\": f\"{row.best_model}({model_names_mapping(row.model)})\",\n",
    "                    \"data\": list(map(math.ceil, row.to_list()[4:-2])),\n",
    "                    \"metrics\": {\n",
    "                        \"mape\": row.mape,\n",
    "                    }\n",
    "                })\n",
    "\n",
    "\n",
    "    logging.info(\"error_sku + \"+str(error_sku))\n",
    "    for k,v in out_json.items():\n",
    "        if k == \"dates\":\n",
    "            continue\n",
    "        out_json[k][\"models\"] = sorted(out_json[k][\"models\"], key=lambda x: x[\"metrics\"][\"mape\"])\n",
    "\n",
    "    output_all = change_name(output_all)\n",
    "    return out_json, output_all\n",
    "\n",
    "data,output_all = training(datasets, forecast_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all.to_csv('meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
